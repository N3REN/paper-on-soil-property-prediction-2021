{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Calcium content (first notebook).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mjVetaupqOKO",
        "C8dYpEw36jT4",
        "qCUXm1SdtUXy",
        "KjQjcEAmqYr7",
        "KEz8Qu0otEj9",
        "s7LyWwqsLT5e",
        "5h0SJurekITD",
        "6NaVQFTqU8Gz"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W4xaWy52nFB",
        "outputId": "d1b59fb4-bde9-4378-8be3-9c8720da13ea"
      },
      "source": [
        "#Attempt to save progress to google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/SaveProgress/'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8z__F02eHD6"
      },
      "source": [
        "Import Libraries/Modules/Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5FDZJ6xUcyj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#import io #handle uploaded files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnEs-mQC2DqL"
      },
      "source": [
        "Link Google drive for easy upload of files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUhmlaG62Kpz"
      },
      "source": [
        "# Code to read csv file from Google Drive into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGA8u7VT3U7W"
      },
      "source": [
        "---\n",
        "Link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AosPwY-r3T7E"
      },
      "source": [
        "###Remove id2 from every cell before running the code in it!\n",
        "\n",
        "link = \"https://drive.google.com/file/d/1VKsRCSz-RLD8eeX_b_FvTAIl0Ika8VrT/view?usp=sharing\" #training.csv share link\n",
        "#id = \"1VKsRCSz-RLD8eeX_b_FvTAIl0Ika8VrT\" #training\n",
        "#id2 = \"1_f8pMy8S75bLrVIi8sFAC85VhhtgRPNp\" #sorted_test\n",
        "print(id) # Verify that you have everything after '='"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieh4vxgcgFxJ"
      },
      "source": [
        "---\n",
        "Attempt to upload dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMTD-AoDgJIL"
      },
      "source": [
        "#Training set\n",
        "id = \"1VKsRCSz-RLD8eeX_b_FvTAIl0Ika8VrT\"\n",
        "train = drive.CreateFile({'id':id}) \n",
        "train.GetContentFile('training.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ5qts6ZoJGb"
      },
      "source": [
        "Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxmtDIHxoT_B"
      },
      "source": [
        "df = pd.read_csv('training.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjVetaupqOKO"
      },
      "source": [
        "---\n",
        
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "4x13U_eprFjt",
        "outputId": "35b0d4f5-af9a-4ae5-fc7a-c8e4f2d10ea8"
      },
      "source": [
        "'''#Determine the number of unique values for each parameter\n",
        "dict = {}\n",
        "for element in list(df.columns):\n",
        "  dict[element] = df[element].value_counts().shape[0]\n",
        "\n",
        "pd.DataFrame(dict, index = [\"unique count\"]).transpose()\n",
        "'''\n",
        "\n",
        "#Shape of data\n",
        "print(\"Dataset Shape (rows, columns): {}\".format(df.shape), '\\n')\n",
        "#Preview\n",
        "print(\"Preview of the first 5 rows for some columns: \")\n",
        "df.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape (rows, columns): (1157, 3600) \n",
            "\n",
            "Preview of the first 5 rows for some columns: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PIDN</th>\n",
              "      <th>m7497.96</th>\n",
              "      <th>m7496.04</th>\n",
              "      <th>m7494.11</th>\n",
              "      <th>m7492.18</th>\n",
              "      <th>m7490.25</th>\n",
              "      <th>m7488.32</th>\n",
              "      <th>m7486.39</th>\n",
              "      <th>m7484.46</th>\n",
              "      <th>m7482.54</th>\n",
              "      <th>m7480.61</th>\n",
              "      <th>m7478.68</th>\n",
              "      <th>m7476.75</th>\n",
              "      <th>m7474.82</th>\n",
              "      <th>m7472.89</th>\n",
              "      <th>m7470.97</th>\n",
              "      <th>m7469.04</th>\n",
              "      <th>m7467.11</th>\n",
              "      <th>m7465.18</th>\n",
              "      <th>m7463.25</th>\n",
              "      <th>m7461.32</th>\n",
              "      <th>m7459.39</th>\n",
              "      <th>m7457.47</th>\n",
              "      <th>m7455.54</th>\n",
              "      <th>m7453.61</th>\n",
              "      <th>m7451.68</th>\n",
              "      <th>m7449.75</th>\n",
              "      <th>m7447.82</th>\n",
              "      <th>m7445.89</th>\n",
              "      <th>m7443.97</th>\n",
              "      <th>m7442.04</th>\n",
              "      <th>m7440.11</th>\n",
              "      <th>m7438.18</th>\n",
              "      <th>m7436.25</th>\n",
              "      <th>m7434.32</th>\n",
              "      <th>m7432.4</th>\n",
              "      <th>m7430.47</th>\n",
              "      <th>m7428.54</th>\n",
              "      <th>m7426.61</th>\n",
              "      <th>m7424.68</th>\n",
              "      <th>...</th>\n",
              "      <th>m634.473</th>\n",
              "      <th>m632.544</th>\n",
              "      <th>m630.616</th>\n",
              "      <th>m628.687</th>\n",
              "      <th>m626.759</th>\n",
              "      <th>m624.83</th>\n",
              "      <th>m622.902</th>\n",
              "      <th>m620.973</th>\n",
              "      <th>m619.045</th>\n",
              "      <th>m617.116</th>\n",
              "      <th>m615.188</th>\n",
              "      <th>m613.259</th>\n",
              "      <th>m611.331</th>\n",
              "      <th>m609.402</th>\n",
              "      <th>m607.474</th>\n",
              "      <th>m605.545</th>\n",
              "      <th>m603.617</th>\n",
              "      <th>m601.688</th>\n",
              "      <th>m599.76</th>\n",
              "      <th>BSAN</th>\n",
              "      <th>BSAS</th>\n",
              "      <th>BSAV</th>\n",
              "      <th>CTI</th>\n",
              "      <th>ELEV</th>\n",
              "      <th>EVI</th>\n",
              "      <th>LSTD</th>\n",
              "      <th>LSTN</th>\n",
              "      <th>REF1</th>\n",
              "      <th>REF2</th>\n",
              "      <th>REF3</th>\n",
              "      <th>REF7</th>\n",
              "      <th>RELI</th>\n",
              "      <th>TMAP</th>\n",
              "      <th>TMFI</th>\n",
              "      <th>Depth</th>\n",
              "      <th>Ca</th>\n",
              "      <th>P</th>\n",
              "      <th>pH</th>\n",
              "      <th>SOC</th>\n",
              "      <th>Sand</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XNhoFZW5</td>\n",
              "      <td>0.302553</td>\n",
              "      <td>0.301137</td>\n",
              "      <td>0.299748</td>\n",
              "      <td>0.300354</td>\n",
              "      <td>0.302679</td>\n",
              "      <td>0.303799</td>\n",
              "      <td>0.301702</td>\n",
              "      <td>0.298936</td>\n",
              "      <td>0.298126</td>\n",
              "      <td>0.298120</td>\n",
              "      <td>0.298163</td>\n",
              "      <td>0.299124</td>\n",
              "      <td>0.300828</td>\n",
              "      <td>0.302522</td>\n",
              "      <td>0.303633</td>\n",
              "      <td>0.303364</td>\n",
              "      <td>0.302018</td>\n",
              "      <td>0.301226</td>\n",
              "      <td>0.300803</td>\n",
              "      <td>0.299270</td>\n",
              "      <td>0.297354</td>\n",
              "      <td>0.296703</td>\n",
              "      <td>0.297569</td>\n",
              "      <td>0.298991</td>\n",
              "      <td>0.299680</td>\n",
              "      <td>0.299230</td>\n",
              "      <td>0.298567</td>\n",
              "      <td>0.298865</td>\n",
              "      <td>0.299278</td>\n",
              "      <td>0.298186</td>\n",
              "      <td>0.296781</td>\n",
              "      <td>0.296565</td>\n",
              "      <td>0.296190</td>\n",
              "      <td>0.294805</td>\n",
              "      <td>0.293779</td>\n",
              "      <td>0.293980</td>\n",
              "      <td>0.295162</td>\n",
              "      <td>0.297448</td>\n",
              "      <td>0.300198</td>\n",
              "      <td>...</td>\n",
              "      <td>1.91489</td>\n",
              "      <td>1.91967</td>\n",
              "      <td>1.91974</td>\n",
              "      <td>1.91909</td>\n",
              "      <td>1.92077</td>\n",
              "      <td>1.91855</td>\n",
              "      <td>1.90573</td>\n",
              "      <td>1.88994</td>\n",
              "      <td>1.87770</td>\n",
              "      <td>1.86431</td>\n",
              "      <td>1.84816</td>\n",
              "      <td>1.83288</td>\n",
              "      <td>1.81858</td>\n",
              "      <td>1.80247</td>\n",
              "      <td>1.78462</td>\n",
              "      <td>1.76644</td>\n",
              "      <td>1.75086</td>\n",
              "      <td>1.74335</td>\n",
              "      <td>1.74246</td>\n",
              "      <td>-0.630435</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>-0.783875</td>\n",
              "      <td>-0.364146</td>\n",
              "      <td>1.165479</td>\n",
              "      <td>1.062682</td>\n",
              "      <td>-0.716713</td>\n",
              "      <td>-0.090016</td>\n",
              "      <td>-0.861091</td>\n",
              "      <td>-0.537106</td>\n",
              "      <td>-0.722567</td>\n",
              "      <td>-0.646673</td>\n",
              "      <td>1.687734</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>Topsoil</td>\n",
              "      <td>-0.295749</td>\n",
              "      <td>-0.041336</td>\n",
              "      <td>-1.129366</td>\n",
              "      <td>0.353258</td>\n",
              "      <td>1.269748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9XNspFTd</td>\n",
              "      <td>0.270192</td>\n",
              "      <td>0.268555</td>\n",
              "      <td>0.266964</td>\n",
              "      <td>0.267938</td>\n",
              "      <td>0.271013</td>\n",
              "      <td>0.272346</td>\n",
              "      <td>0.269870</td>\n",
              "      <td>0.266976</td>\n",
              "      <td>0.266544</td>\n",
              "      <td>0.266766</td>\n",
              "      <td>0.266464</td>\n",
              "      <td>0.266817</td>\n",
              "      <td>0.268150</td>\n",
              "      <td>0.269933</td>\n",
              "      <td>0.271409</td>\n",
              "      <td>0.271396</td>\n",
              "      <td>0.270126</td>\n",
              "      <td>0.269351</td>\n",
              "      <td>0.268984</td>\n",
              "      <td>0.267680</td>\n",
              "      <td>0.265901</td>\n",
              "      <td>0.265088</td>\n",
              "      <td>0.265679</td>\n",
              "      <td>0.266744</td>\n",
              "      <td>0.267202</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.266266</td>\n",
              "      <td>0.266768</td>\n",
              "      <td>0.267507</td>\n",
              "      <td>0.266740</td>\n",
              "      <td>0.265624</td>\n",
              "      <td>0.265355</td>\n",
              "      <td>0.264461</td>\n",
              "      <td>0.262806</td>\n",
              "      <td>0.262251</td>\n",
              "      <td>0.263087</td>\n",
              "      <td>0.264431</td>\n",
              "      <td>0.266533</td>\n",
              "      <td>0.269126</td>\n",
              "      <td>...</td>\n",
              "      <td>2.00603</td>\n",
              "      <td>2.00192</td>\n",
              "      <td>2.00225</td>\n",
              "      <td>2.00244</td>\n",
              "      <td>1.99688</td>\n",
              "      <td>1.98540</td>\n",
              "      <td>1.96969</td>\n",
              "      <td>1.94942</td>\n",
              "      <td>1.92816</td>\n",
              "      <td>1.91071</td>\n",
              "      <td>1.89728</td>\n",
              "      <td>1.88298</td>\n",
              "      <td>1.86131</td>\n",
              "      <td>1.83355</td>\n",
              "      <td>1.80581</td>\n",
              "      <td>1.78410</td>\n",
              "      <td>1.77195</td>\n",
              "      <td>1.76479</td>\n",
              "      <td>1.75437</td>\n",
              "      <td>-0.630435</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>-0.783875</td>\n",
              "      <td>-0.364146</td>\n",
              "      <td>1.165479</td>\n",
              "      <td>1.062682</td>\n",
              "      <td>-0.716713</td>\n",
              "      <td>-0.090016</td>\n",
              "      <td>-0.861091</td>\n",
              "      <td>-0.537106</td>\n",
              "      <td>-0.722567</td>\n",
              "      <td>-0.646673</td>\n",
              "      <td>1.687734</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>Subsoil</td>\n",
              "      <td>-0.387442</td>\n",
              "      <td>-0.231552</td>\n",
              "      <td>-1.531538</td>\n",
              "      <td>-0.264023</td>\n",
              "      <td>1.692209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WDId41qG</td>\n",
              "      <td>0.317433</td>\n",
              "      <td>0.316265</td>\n",
              "      <td>0.314948</td>\n",
              "      <td>0.315224</td>\n",
              "      <td>0.316942</td>\n",
              "      <td>0.317764</td>\n",
              "      <td>0.316067</td>\n",
              "      <td>0.313874</td>\n",
              "      <td>0.313301</td>\n",
              "      <td>0.313296</td>\n",
              "      <td>0.313051</td>\n",
              "      <td>0.313306</td>\n",
              "      <td>0.314301</td>\n",
              "      <td>0.315640</td>\n",
              "      <td>0.316764</td>\n",
              "      <td>0.316759</td>\n",
              "      <td>0.315631</td>\n",
              "      <td>0.314860</td>\n",
              "      <td>0.314275</td>\n",
              "      <td>0.312711</td>\n",
              "      <td>0.311094</td>\n",
              "      <td>0.310565</td>\n",
              "      <td>0.311120</td>\n",
              "      <td>0.312103</td>\n",
              "      <td>0.312638</td>\n",
              "      <td>0.312326</td>\n",
              "      <td>0.311623</td>\n",
              "      <td>0.311752</td>\n",
              "      <td>0.312137</td>\n",
              "      <td>0.311122</td>\n",
              "      <td>0.309909</td>\n",
              "      <td>0.309824</td>\n",
              "      <td>0.309471</td>\n",
              "      <td>0.308209</td>\n",
              "      <td>0.307262</td>\n",
              "      <td>0.307201</td>\n",
              "      <td>0.307804</td>\n",
              "      <td>0.309592</td>\n",
              "      <td>0.312165</td>\n",
              "      <td>...</td>\n",
              "      <td>1.79495</td>\n",
              "      <td>1.79606</td>\n",
              "      <td>1.79749</td>\n",
              "      <td>1.79798</td>\n",
              "      <td>1.79977</td>\n",
              "      <td>1.80183</td>\n",
              "      <td>1.80012</td>\n",
              "      <td>1.79366</td>\n",
              "      <td>1.78411</td>\n",
              "      <td>1.77356</td>\n",
              "      <td>1.76544</td>\n",
              "      <td>1.76124</td>\n",
              "      <td>1.75742</td>\n",
              "      <td>1.75113</td>\n",
              "      <td>1.74128</td>\n",
              "      <td>1.72894</td>\n",
              "      <td>1.71991</td>\n",
              "      <td>1.71562</td>\n",
              "      <td>1.71158</td>\n",
              "      <td>-0.753623</td>\n",
              "      <td>-0.836364</td>\n",
              "      <td>-0.929451</td>\n",
              "      <td>-0.633972</td>\n",
              "      <td>1.544098</td>\n",
              "      <td>1.156705</td>\n",
              "      <td>-1.282552</td>\n",
              "      <td>-0.088336</td>\n",
              "      <td>-0.935273</td>\n",
              "      <td>-0.631725</td>\n",
              "      <td>-0.832298</td>\n",
              "      <td>-0.814516</td>\n",
              "      <td>1.806660</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>Topsoil</td>\n",
              "      <td>-0.248601</td>\n",
              "      <td>-0.224635</td>\n",
              "      <td>-0.259551</td>\n",
              "      <td>0.064152</td>\n",
              "      <td>2.091835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JrrJf1mN</td>\n",
              "      <td>0.261116</td>\n",
              "      <td>0.259767</td>\n",
              "      <td>0.258384</td>\n",
              "      <td>0.259001</td>\n",
              "      <td>0.261310</td>\n",
              "      <td>0.262417</td>\n",
              "      <td>0.260534</td>\n",
              "      <td>0.258039</td>\n",
              "      <td>0.257246</td>\n",
              "      <td>0.257124</td>\n",
              "      <td>0.257018</td>\n",
              "      <td>0.257568</td>\n",
              "      <td>0.258724</td>\n",
              "      <td>0.260107</td>\n",
              "      <td>0.261175</td>\n",
              "      <td>0.261028</td>\n",
              "      <td>0.259906</td>\n",
              "      <td>0.259251</td>\n",
              "      <td>0.258669</td>\n",
              "      <td>0.257007</td>\n",
              "      <td>0.255397</td>\n",
              "      <td>0.255119</td>\n",
              "      <td>0.256042</td>\n",
              "      <td>0.257195</td>\n",
              "      <td>0.257301</td>\n",
              "      <td>0.256440</td>\n",
              "      <td>0.256007</td>\n",
              "      <td>0.256729</td>\n",
              "      <td>0.257216</td>\n",
              "      <td>0.256001</td>\n",
              "      <td>0.254599</td>\n",
              "      <td>0.254345</td>\n",
              "      <td>0.253791</td>\n",
              "      <td>0.252452</td>\n",
              "      <td>0.251695</td>\n",
              "      <td>0.252027</td>\n",
              "      <td>0.253043</td>\n",
              "      <td>0.254901</td>\n",
              "      <td>0.257175</td>\n",
              "      <td>...</td>\n",
              "      <td>1.75317</td>\n",
              "      <td>1.76090</td>\n",
              "      <td>1.76944</td>\n",
              "      <td>1.77287</td>\n",
              "      <td>1.77080</td>\n",
              "      <td>1.76396</td>\n",
              "      <td>1.75453</td>\n",
              "      <td>1.74775</td>\n",
              "      <td>1.74264</td>\n",
              "      <td>1.73527</td>\n",
              "      <td>1.72770</td>\n",
              "      <td>1.72349</td>\n",
              "      <td>1.72149</td>\n",
              "      <td>1.71630</td>\n",
              "      <td>1.70737</td>\n",
              "      <td>1.69952</td>\n",
              "      <td>1.69356</td>\n",
              "      <td>1.68812</td>\n",
              "      <td>1.68178</td>\n",
              "      <td>-0.753623</td>\n",
              "      <td>-0.836364</td>\n",
              "      <td>-0.929451</td>\n",
              "      <td>-0.633972</td>\n",
              "      <td>1.544098</td>\n",
              "      <td>1.156705</td>\n",
              "      <td>-1.282552</td>\n",
              "      <td>-0.088336</td>\n",
              "      <td>-0.935273</td>\n",
              "      <td>-0.631725</td>\n",
              "      <td>-0.832298</td>\n",
              "      <td>-0.814516</td>\n",
              "      <td>1.806660</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>Subsoil</td>\n",
              "      <td>-0.332195</td>\n",
              "      <td>-0.318014</td>\n",
              "      <td>-0.577548</td>\n",
              "      <td>-0.318719</td>\n",
              "      <td>2.118477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ZoIitegA</td>\n",
              "      <td>0.260038</td>\n",
              "      <td>0.258425</td>\n",
              "      <td>0.256544</td>\n",
              "      <td>0.257030</td>\n",
              "      <td>0.259602</td>\n",
              "      <td>0.260786</td>\n",
              "      <td>0.258717</td>\n",
              "      <td>0.256352</td>\n",
              "      <td>0.255902</td>\n",
              "      <td>0.255822</td>\n",
              "      <td>0.255720</td>\n",
              "      <td>0.256521</td>\n",
              "      <td>0.257968</td>\n",
              "      <td>0.259571</td>\n",
              "      <td>0.260714</td>\n",
              "      <td>0.260465</td>\n",
              "      <td>0.259352</td>\n",
              "      <td>0.258872</td>\n",
              "      <td>0.258484</td>\n",
              "      <td>0.257105</td>\n",
              "      <td>0.255502</td>\n",
              "      <td>0.254720</td>\n",
              "      <td>0.255194</td>\n",
              "      <td>0.256394</td>\n",
              "      <td>0.257036</td>\n",
              "      <td>0.256583</td>\n",
              "      <td>0.255867</td>\n",
              "      <td>0.256101</td>\n",
              "      <td>0.256414</td>\n",
              "      <td>0.255297</td>\n",
              "      <td>0.254055</td>\n",
              "      <td>0.253889</td>\n",
              "      <td>0.253455</td>\n",
              "      <td>0.252198</td>\n",
              "      <td>0.251296</td>\n",
              "      <td>0.251400</td>\n",
              "      <td>0.252441</td>\n",
              "      <td>0.254763</td>\n",
              "      <td>0.257593</td>\n",
              "      <td>...</td>\n",
              "      <td>1.74973</td>\n",
              "      <td>1.75710</td>\n",
              "      <td>1.76209</td>\n",
              "      <td>1.76110</td>\n",
              "      <td>1.75564</td>\n",
              "      <td>1.75006</td>\n",
              "      <td>1.74568</td>\n",
              "      <td>1.74050</td>\n",
              "      <td>1.73201</td>\n",
              "      <td>1.72088</td>\n",
              "      <td>1.70944</td>\n",
              "      <td>1.69711</td>\n",
              "      <td>1.68257</td>\n",
              "      <td>1.66762</td>\n",
              "      <td>1.65639</td>\n",
              "      <td>1.64929</td>\n",
              "      <td>1.64089</td>\n",
              "      <td>1.62805</td>\n",
              "      <td>1.61643</td>\n",
              "      <td>-0.688406</td>\n",
              "      <td>-0.763636</td>\n",
              "      <td>-0.884658</td>\n",
              "      <td>-0.583576</td>\n",
              "      <td>1.276837</td>\n",
              "      <td>1.191691</td>\n",
              "      <td>-1.206971</td>\n",
              "      <td>0.011420</td>\n",
              "      <td>-0.906182</td>\n",
              "      <td>-0.528757</td>\n",
              "      <td>-0.795031</td>\n",
              "      <td>-0.780242</td>\n",
              "      <td>0.430513</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>Topsoil</td>\n",
              "      <td>-0.438350</td>\n",
              "      <td>-0.010210</td>\n",
              "      <td>-0.699135</td>\n",
              "      <td>-0.310905</td>\n",
              "      <td>2.164148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 3600 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       PIDN  m7497.96  m7496.04  ...        pH       SOC      Sand\n",
              "0  XNhoFZW5  0.302553  0.301137  ... -1.129366  0.353258  1.269748\n",
              "1  9XNspFTd  0.270192  0.268555  ... -1.531538 -0.264023  1.692209\n",
              "2  WDId41qG  0.317433  0.316265  ... -0.259551  0.064152  2.091835\n",
              "3  JrrJf1mN  0.261116  0.259767  ... -0.577548 -0.318719  2.118477\n",
              "4  ZoIitegA  0.260038  0.258425  ... -0.699135 -0.310905  2.164148\n",
              "\n",
              "[5 rows x 3600 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUw53g5frAww"
      },
      "source": [
        "Attempting to split data into categorical and numerical (continuous) sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "OEyuAW5RrOjM",
        "outputId": "f59da349-ce5c-4eed-9ea6-301e14c06848"
      },
      "source": [
        "#Data is entirely numerical \n",
        "#Excluding Depth of the soil sample ('Depth') (2 categories: \"Topsoil\", \"Subsoil\")\n",
        "#and PIDN which are sample identifiers\n",
        "\n",
        "#making a copy of DataFrame (df) just to be safe\n",
        "data = df\n",
        "#split\n",
        "categorical = ['Depth']\n",
        "continuous = data.drop(['PIDN', 'Depth'], axis = 1)\n",
        "continuous.head()\n",
        "#print(list(continuous.columns)) #to get a list of all continuous data columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbod tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m7497.96</th>\n",
              "      <th>m7496.04</th>\n",
              "      <th>m7494.11</th>\n",
              "      <th>m7492.18</th>\n",
              "      <th>m7490.25</th>\n",
              "      <th>m7488.32</th>\n",
              "      <th>m7486.39</th>\n",
              "      <th>m7484.46</th>\n",
              "      <th>m7482.54</th>\n",
              "      <th>m7480.61</th>\n",
              "      <th>m7478.68</th>\n",
              "      <th>m7476.75</th>\n",
              "      <th>m7474.82</th>\n",
              "      <th>m7472.89</th>\n",
              "      <th>m7470.97</th>\n",
              "      <th>m7469.04</th>\n",
              "      <th>m7467.11</th>\n",
              "      <th>m7465.18</th>\n",
              "      <th>m7463.25</th>\n",
              "      <th>m7461.32</th>\n",
              "      <th>m7459.39</th>\n",
              "      <th>m7457.47</th>\n",
              "      <th>m7455.54</th>\n",
              "      <th>m7453.61</th>\n",
              "      <th>m7451.68</th>\n",
              "      <th>m7449.75</th>\n",
              "      <th>m7447.82</th>\n",
              "      <th>m7445.89</th>\n",
              "      <th>m7443.97</th>\n",
              "      <th>m7442.04</th>\n",
              "      <th>m7440.11</th>\n",
              "      <th>m7438.18</th>\n",
              "      <th>m7436.25</th>\n",
              "      <th>m7434.32</th>\n",
              "      <th>m7432.4</th>\n",
              "      <th>m7430.47</th>\n",
              "      <th>m7428.54</th>\n",
              "      <th>m7426.61</th>\n",
              "      <th>m7424.68</th>\n",
              "      <th>m7422.75</th>\n",
              "      <th>...</th>\n",
              "      <th>m636.401</th>\n",
              "      <th>m634.473</th>\n",
              "      <th>m632.544</th>\n",
              "      <th>m630.616</th>\n",
              "      <th>m628.687</th>\n",
              "      <th>m626.759</th>\n",
              "      <th>m624.83</th>\n",
              "      <th>m622.902</th>\n",
              "      <th>m620.973</th>\n",
              "      <th>m619.045</th>\n",
              "      <th>m617.116</th>\n",
              "      <th>m615.188</th>\n",
              "      <th>m613.259</th>\n",
              "      <th>m611.331</th>\n",
              "      <th>m609.402</th>\n",
              "      <th>m607.474</th>\n",
              "      <th>m605.545</th>\n",
              "      <th>m603.617</th>\n",
              "      <th>m601.688</th>\n",
              "      <th>m599.76</th>\n",
              "      <th>BSAN</th>\n",
              "      <th>BSAS</th>\n",
              "      <th>BSAV</th>\n",
              "      <th>CTI</th>\n",
              "      <th>ELEV</th>\n",
              "      <th>EVI</th>\n",
              "      <th>LSTD</th>\n",
              "      <th>LSTN</th>\n",
              "      <th>REF1</th>\n",
              "      <th>REF2</th>\n",
              "      <th>REF3</th>\n",
              "      <th>REF7</th>\n",
              "      <th>RELI</th>\n",
              "      <th>TMAP</th>\n",
              "      <th>TMFI</th>\n",
              "      <th>Ca</th>\n",
              "      <th>P</th>\n",
              "      <th>pH</th>\n",
              "      <th>SOC</th>\n",
              "      <th>Sand</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.302553</td>\n",
              "      <td>0.301137</td>\n",
              "      <td>0.299748</td>\n",
              "      <td>0.300354</td>\n",
              "      <td>0.302679</td>\n",
              "      <td>0.303799</td>\n",
              "      <td>0.301702</td>\n",
              "      <td>0.298936</td>\n",
              "      <td>0.298126</td>\n",
              "      <td>0.298120</td>\n",
              "      <td>0.298163</td>\n",
              "      <td>0.299124</td>\n",
              "      <td>0.300828</td>\n",
              "      <td>0.302522</td>\n",
              "      <td>0.303633</td>\n",
              "      <td>0.303364</td>\n",
              "      <td>0.302018</td>\n",
              "      <td>0.301226</td>\n",
              "      <td>0.300803</td>\n",
              "      <td>0.299270</td>\n",
              "      <td>0.297354</td>\n",
              "      <td>0.296703</td>\n",
              "      <td>0.297569</td>\n",
              "      <td>0.298991</td>\n",
              "      <td>0.299680</td>\n",
              "      <td>0.299230</td>\n",
              "      <td>0.298567</td>\n",
              "      <td>0.298865</td>\n",
              "      <td>0.299278</td>\n",
              "      <td>0.298186</td>\n",
              "      <td>0.296781</td>\n",
              "      <td>0.296565</td>\n",
              "      <td>0.296190</td>\n",
              "      <td>0.294805</td>\n",
              "      <td>0.293779</td>\n",
              "      <td>0.293980</td>\n",
              "      <td>0.295162</td>\n",
              "      <td>0.297448</td>\n",
              "      <td>0.300198</td>\n",
              "      <td>0.301303</td>\n",
              "      <td>...</td>\n",
              "      <td>1.90753</td>\n",
              "      <td>1.91489</td>\n",
              "      <td>1.91967</td>\n",
              "      <td>1.91974</td>\n",
              "      <td>1.91909</td>\n",
              "      <td>1.92077</td>\n",
              "      <td>1.91855</td>\n",
              "      <td>1.90573</td>\n",
              "      <td>1.88994</td>\n",
              "      <td>1.87770</td>\n",
              "      <td>1.86431</td>\n",
              "      <td>1.84816</td>\n",
              "      <td>1.83288</td>\n",
              "      <td>1.81858</td>\n",
              "      <td>1.80247</td>\n",
              "      <td>1.78462</td>\n",
              "      <td>1.76644</td>\n",
              "      <td>1.75086</td>\n",
              "      <td>1.74335</td>\n",
              "      <td>1.74246</td>\n",
              "      <td>-0.630435</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>-0.783875</td>\n",
              "      <td>-0.364146</td>\n",
              "      <td>1.165479</td>\n",
              "      <td>1.062682</td>\n",
              "      <td>-0.716713</td>\n",
              "      <td>-0.090016</td>\n",
              "      <td>-0.861091</td>\n",
              "      <td>-0.537106</td>\n",
              "      <td>-0.722567</td>\n",
              "      <td>-0.646673</td>\n",
              "      <td>1.687734</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>-0.295749</td>\n",
              "      <td>-0.041336</td>\n",
              "      <td>-1.129366</td>\n",
              "      <td>0.353258</td>\n",
              "      <td>1.269748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.270192</td>\n",
              "      <td>0.268555</td>\n",
              "      <td>0.266964</td>\n",
              "      <td>0.267938</td>\n",
              "      <td>0.271013</td>\n",
              "      <td>0.272346</td>\n",
              "      <td>0.269870</td>\n",
              "      <td>0.266976</td>\n",
              "      <td>0.266544</td>\n",
              "      <td>0.266766</td>\n",
              "      <td>0.266464</td>\n",
              "      <td>0.266817</td>\n",
              "      <td>0.268150</td>\n",
              "      <td>0.269933</td>\n",
              "      <td>0.271409</td>\n",
              "      <td>0.271396</td>\n",
              "      <td>0.270126</td>\n",
              "      <td>0.269351</td>\n",
              "      <td>0.268984</td>\n",
              "      <td>0.267680</td>\n",
              "      <td>0.265901</td>\n",
              "      <td>0.265088</td>\n",
              "      <td>0.265679</td>\n",
              "      <td>0.266744</td>\n",
              "      <td>0.267202</td>\n",
              "      <td>0.266808</td>\n",
              "      <td>0.266266</td>\n",
              "      <td>0.266768</td>\n",
              "      <td>0.267507</td>\n",
              "      <td>0.266740</td>\n",
              "      <td>0.265624</td>\n",
              "      <td>0.265355</td>\n",
              "      <td>0.264461</td>\n",
              "      <td>0.262806</td>\n",
              "      <td>0.262251</td>\n",
              "      <td>0.263087</td>\n",
              "      <td>0.264431</td>\n",
              "      <td>0.266533</td>\n",
              "      <td>0.269126</td>\n",
              "      <td>0.270218</td>\n",
              "      <td>...</td>\n",
              "      <td>2.01296</td>\n",
              "      <td>2.00603</td>\n",
              "      <td>2.00192</td>\n",
              "      <td>2.00225</td>\n",
              "      <td>2.00244</td>\n",
              "      <td>1.99688</td>\n",
              "      <td>1.98540</td>\n",
              "      <td>1.96969</td>\n",
              "      <td>1.94942</td>\n",
              "      <td>1.92816</td>\n",
              "      <td>1.91071</td>\n",
              "      <td>1.89728</td>\n",
              "      <td>1.88298</td>\n",
              "      <td>1.86131</td>\n",
              "      <td>1.83355</td>\n",
              "      <td>1.80581</td>\n",
              "      <td>1.78410</td>\n",
              "      <td>1.77195</td>\n",
              "      <td>1.76479</td>\n",
              "      <td>1.75437</td>\n",
              "      <td>-0.630435</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>-0.783875</td>\n",
              "      <td>-0.364146</td>\n",
              "      <td>1.165479</td>\n",
              "      <td>1.062682</td>\n",
              "      <td>-0.716713</td>\n",
              "      <td>-0.090016</td>\n",
              "      <td>-0.861091</td>\n",
              "      <td>-0.537106</td>\n",
              "      <td>-0.722567</td>\n",
              "      <td>-0.646673</td>\n",
              "      <td>1.687734</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>-0.387442</td>\n",
              "      <td>-0.231552</td>\n",
              "      <td>-1.531538</td>\n",
              "      <td>-0.264023</td>\n",
              "      <td>1.692209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.317433</td>\n",
              "      <td>0.316265</td>\n",
              "      <td>0.314948</td>\n",
              "      <td>0.315224</td>\n",
              "      <td>0.316942</td>\n",
              "      <td>0.317764</td>\n",
              "      <td>0.316067</td>\n",
              "      <td>0.313874</td>\n",
              "      <td>0.313301</td>\n",
              "      <td>0.313296</td>\n",
              "      <td>0.313051</td>\n",
              "      <td>0.313306</td>\n",
              "      <td>0.314301</td>\n",
              "      <td>0.315640</td>\n",
              "      <td>0.316764</td>\n",
              "      <td>0.316759</td>\n",
              "      <td>0.315631</td>\n",
              "      <td>0.314860</td>\n",
              "      <td>0.314275</td>\n",
              "      <td>0.312711</td>\n",
              "      <td>0.311094</td>\n",
              "      <td>0.310565</td>\n",
              "      <td>0.311120</td>\n",
              "      <td>0.312103</td>\n",
              "      <td>0.312638</td>\n",
              "      <td>0.312326</td>\n",
              "      <td>0.311623</td>\n",
              "      <td>0.311752</td>\n",
              "      <td>0.312137</td>\n",
              "      <td>0.311122</td>\n",
              "      <td>0.309909</td>\n",
              "      <td>0.309824</td>\n",
              "      <td>0.309471</td>\n",
              "      <td>0.308209</td>\n",
              "      <td>0.307262</td>\n",
              "      <td>0.307201</td>\n",
              "      <td>0.307804</td>\n",
              "      <td>0.309592</td>\n",
              "      <td>0.312165</td>\n",
              "      <td>0.313208</td>\n",
              "      <td>...</td>\n",
              "      <td>1.79348</td>\n",
              "      <td>1.79495</td>\n",
              "      <td>1.79606</td>\n",
              "      <td>1.79749</td>\n",
              "      <td>1.79798</td>\n",
              "      <td>1.79977</td>\n",
              "      <td>1.80183</td>\n",
              "      <td>1.80012</td>\n",
              "      <td>1.79366</td>\n",
              "      <td>1.78411</td>\n",
              "      <td>1.77356</td>\n",
              "      <td>1.76544</td>\n",
              "      <td>1.76124</td>\n",
              "      <td>1.75742</td>\n",
              "      <td>1.75113</td>\n",
              "      <td>1.74128</td>\n",
              "      <td>1.72894</td>\n",
              "      <td>1.71991</td>\n",
              "      <td>1.71562</td>\n",
              "      <td>1.71158</td>\n",
              "      <td>-0.753623</td>\n",
              "      <td>-0.836364</td>\n",
              "      <td>-0.929451</td>\n",
              "      <td>-0.633972</td>\n",
              "      <td>1.544098</td>\n",
              "      <td>1.156705</td>\n",
              "      <td>-1.282552</td>\n",
              "      <td>-0.088336</td>\n",
              "      <td>-0.935273</td>\n",
              "      <td>-0.631725</td>\n",
              "      <td>-0.832298</td>\n",
              "      <td>-0.814516</td>\n",
              "      <td>1.806660</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>-0.248601</td>\n",
              "      <td>-0.224635</td>\n",
              "      <td>-0.259551</td>\n",
              "      <td>0.064152</td>\n",
              "      <td>2.091835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.261116</td>\n",
              "      <td>0.259767</td>\n",
              "      <td>0.258384</td>\n",
              "      <td>0.259001</td>\n",
              "      <td>0.261310</td>\n",
              "      <td>0.262417</td>\n",
              "      <td>0.260534</td>\n",
              "      <td>0.258039</td>\n",
              "      <td>0.257246</td>\n",
              "      <td>0.257124</td>\n",
              "      <td>0.257018</td>\n",
              "      <td>0.257568</td>\n",
              "      <td>0.258724</td>\n",
              "      <td>0.260107</td>\n",
              "      <td>0.261175</td>\n",
              "      <td>0.261028</td>\n",
              "      <td>0.259906</td>\n",
              "      <td>0.259251</td>\n",
              "      <td>0.258669</td>\n",
              "      <td>0.257007</td>\n",
              "      <td>0.255397</td>\n",
              "      <td>0.255119</td>\n",
              "      <td>0.256042</td>\n",
              "      <td>0.257195</td>\n",
              "      <td>0.257301</td>\n",
              "      <td>0.256440</td>\n",
              "      <td>0.256007</td>\n",
              "      <td>0.256729</td>\n",
              "      <td>0.257216</td>\n",
              "      <td>0.256001</td>\n",
              "      <td>0.254599</td>\n",
              "      <td>0.254345</td>\n",
              "      <td>0.253791</td>\n",
              "      <td>0.252452</td>\n",
              "      <td>0.251695</td>\n",
              "      <td>0.252027</td>\n",
              "      <td>0.253043</td>\n",
              "      <td>0.254901</td>\n",
              "      <td>0.257175</td>\n",
              "      <td>0.258014</td>\n",
              "      <td>...</td>\n",
              "      <td>1.74986</td>\n",
              "      <td>1.75317</td>\n",
              "      <td>1.76090</td>\n",
              "      <td>1.76944</td>\n",
              "      <td>1.77287</td>\n",
              "      <td>1.77080</td>\n",
              "      <td>1.76396</td>\n",
              "      <td>1.75453</td>\n",
              "      <td>1.74775</td>\n",
              "      <td>1.74264</td>\n",
              "      <td>1.73527</td>\n",
              "      <td>1.72770</td>\n",
              "      <td>1.72349</td>\n",
              "      <td>1.72149</td>\n",
              "      <td>1.71630</td>\n",
              "      <td>1.70737</td>\n",
              "      <td>1.69952</td>\n",
              "      <td>1.69356</td>\n",
              "      <td>1.68812</td>\n",
              "      <td>1.68178</td>\n",
              "      <td>-0.753623</td>\n",
              "      <td>-0.836364</td>\n",
              "      <td>-0.929451</td>\n",
              "      <td>-0.633972</td>\n",
              "      <td>1.544098</td>\n",
              "      <td>1.156705</td>\n",
              "      <td>-1.282552</td>\n",
              "      <td>-0.088336</td>\n",
              "      <td>-0.935273</td>\n",
              "      <td>-0.631725</td>\n",
              "      <td>-0.832298</td>\n",
              "      <td>-0.814516</td>\n",
              "      <td>1.806660</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>-0.332195</td>\n",
              "      <td>-0.318014</td>\n",
              "      <td>-0.577548</td>\n",
              "      <td>-0.318719</td>\n",
              "      <td>2.118477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.260038</td>\n",
              "      <td>0.258425</td>\n",
              "      <td>0.256544</td>\n",
              "      <td>0.257030</td>\n",
              "      <td>0.259602</td>\n",
              "      <td>0.260786</td>\n",
              "      <td>0.258717</td>\n",
              "      <td>0.256352</td>\n",
              "      <td>0.255902</td>\n",
              "      <td>0.255822</td>\n",
              "      <td>0.255720</td>\n",
              "      <td>0.256521</td>\n",
              "      <td>0.257968</td>\n",
              "      <td>0.259571</td>\n",
              "      <td>0.260714</td>\n",
              "      <td>0.260465</td>\n",
              "      <td>0.259352</td>\n",
              "      <td>0.258872</td>\n",
              "      <td>0.258484</td>\n",
              "      <td>0.257105</td>\n",
              "      <td>0.255502</td>\n",
              "      <td>0.254720</td>\n",
              "      <td>0.255194</td>\n",
              "      <td>0.256394</td>\n",
              "      <td>0.257036</td>\n",
              "      <td>0.256583</td>\n",
              "      <td>0.255867</td>\n",
              "      <td>0.256101</td>\n",
              "      <td>0.256414</td>\n",
              "      <td>0.255297</td>\n",
              "      <td>0.254055</td>\n",
              "      <td>0.253889</td>\n",
              "      <td>0.253455</td>\n",
              "      <td>0.252198</td>\n",
              "      <td>0.251296</td>\n",
              "      <td>0.251400</td>\n",
              "      <td>0.252441</td>\n",
              "      <td>0.254763</td>\n",
              "      <td>0.257593</td>\n",
              "      <td>0.258576</td>\n",
              "      <td>...</td>\n",
              "      <td>1.74285</td>\n",
              "      <td>1.74973</td>\n",
              "      <td>1.75710</td>\n",
              "      <td>1.76209</td>\n",
              "      <td>1.76110</td>\n",
              "      <td>1.75564</td>\n",
              "      <td>1.75006</td>\n",
              "      <td>1.74568</td>\n",
              "      <td>1.74050</td>\n",
              "      <td>1.73201</td>\n",
              "      <td>1.72088</td>\n",
              "      <td>1.70944</td>\n",
              "      <td>1.69711</td>\n",
              "      <td>1.68257</td>\n",
              "      <td>1.66762</td>\n",
              "      <td>1.65639</td>\n",
              "      <td>1.64929</td>\n",
              "      <td>1.64089</td>\n",
              "      <td>1.62805</td>\n",
              "      <td>1.61643</td>\n",
              "      <td>-0.688406</td>\n",
              "      <td>-0.763636</td>\n",
              "      <td>-0.884658</td>\n",
              "      <td>-0.583576</td>\n",
              "      <td>1.276837</td>\n",
              "      <td>1.191691</td>\n",
              "      <td>-1.206971</td>\n",
              "      <td>0.011420</td>\n",
              "      <td>-0.906182</td>\n",
              "      <td>-0.528757</td>\n",
              "      <td>-0.795031</td>\n",
              "      <td>-0.780242</td>\n",
              "      <td>0.430513</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>-0.438350</td>\n",
              "      <td>-0.010210</td>\n",
              "      <td>-0.699135</td>\n",
              "      <td>-0.310905</td>\n",
              "      <td>2.164148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 3598 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   m7497.96  m7496.04  m7494.11  ...        pH       SOC      Sand\n",
              "0  0.302553  0.301137  0.299748  ... -1.129366  0.353258  1.269748\n",
              "1  0.270192  0.268555  0.266964  ... -1.531538 -0.264023  1.692209\n",
              "2  0.317433  0.316265  0.314948  ... -0.259551  0.064152  2.091835\n",
              "3  0.261116  0.259767  0.258384  ... -0.577548 -0.318719  2.118477\n",
              "4  0.260038  0.258425  0.256544  ... -0.699135 -0.310905  2.164148\n",
              "\n",
              "[5 rows x 3598 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7c7Q8BRohMR"
      },
      "source": [
        "#Overview of Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "rWVtCeHdoPxm",
        "outputId": "03a407dc-44be-4cf0-e7a3-1a5c5da897f9"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m7497.96</th>\n",
              "      <th>m7496.04</th>\n",
              "      <th>m7494.11</th>\n",
              "      <th>m7492.18</th>\n",
              "      <th>m7490.25</th>\n",
              "      <th>m7488.32</th>\n",
              "      <th>m7486.39</th>\n",
              "      <th>m7484.46</th>\n",
              "      <th>m7482.54</th>\n",
              "      <th>m7480.61</th>\n",
              "      <th>m7478.68</th>\n",
              "      <th>m7476.75</th>\n",
              "      <th>m7474.82</th>\n",
              "      <th>m7472.89</th>\n",
              "      <th>m7470.97</th>\n",
              "      <th>m7469.04</th>\n",
              "      <th>m7467.11</th>\n",
              "      <th>m7465.18</th>\n",
              "      <th>m7463.25</th>\n",
              "      <th>m7461.32</th>\n",
              "      <th>m7459.39</th>\n",
              "      <th>m7457.47</th>\n",
              "      <th>m7455.54</th>\n",
              "      <th>m7453.61</th>\n",
              "      <th>m7451.68</th>\n",
              "      <th>m7449.75</th>\n",
              "      <th>m7447.82</th>\n",
              "      <th>m7445.89</th>\n",
              "      <th>m7443.97</th>\n",
              "      <th>m7442.04</th>\n",
              "      <th>m7440.11</th>\n",
              "      <th>m7438.18</th>\n",
              "      <th>m7436.25</th>\n",
              "      <th>m7434.32</th>\n",
              "      <th>m7432.4</th>\n",
              "      <th>m7430.47</th>\n",
              "      <th>m7428.54</th>\n",
              "      <th>m7426.61</th>\n",
              "      <th>m7424.68</th>\n",
              "      <th>m7422.75</th>\n",
              "      <th>...</th>\n",
              "      <th>m636.401</th>\n",
              "      <th>m634.473</th>\n",
              "      <th>m632.544</th>\n",
              "      <th>m630.616</th>\n",
              "      <th>m628.687</th>\n",
              "      <th>m626.759</th>\n",
              "      <th>m624.83</th>\n",
              "      <th>m622.902</th>\n",
              "      <th>m620.973</th>\n",
              "      <th>m619.045</th>\n",
              "      <th>m617.116</th>\n",
              "      <th>m615.188</th>\n",
              "      <th>m613.259</th>\n",
              "      <th>m611.331</th>\n",
              "      <th>m609.402</th>\n",
              "      <th>m607.474</th>\n",
              "      <th>m605.545</th>\n",
              "      <th>m603.617</th>\n",
              "      <th>m601.688</th>\n",
              "      <th>m599.76</th>\n",
              "      <th>BSAN</th>\n",
              "      <th>BSAS</th>\n",
              "      <th>BSAV</th>\n",
              "      <th>CTI</th>\n",
              "      <th>ELEV</th>\n",
              "      <th>EVI</th>\n",
              "      <th>LSTD</th>\n",
              "      <th>LSTN</th>\n",
              "      <th>REF1</th>\n",
              "      <th>REF2</th>\n",
              "      <th>REF3</th>\n",
              "      <th>REF7</th>\n",
              "      <th>RELI</th>\n",
              "      <th>TMAP</th>\n",
              "      <th>TMFI</th>\n",
              "      <th>Ca</th>\n",
              "      <th>P</th>\n",
              "      <th>pH</th>\n",
              "      <th>SOC</th>\n",
              "      <th>Sand</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "      <td>1157.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.245666</td>\n",
              "      <td>0.240454</td>\n",
              "      <td>0.235631</td>\n",
              "      <td>0.238994</td>\n",
              "      <td>0.248176</td>\n",
              "      <td>0.251674</td>\n",
              "      <td>0.243996</td>\n",
              "      <td>0.235162</td>\n",
              "      <td>0.232874</td>\n",
              "      <td>0.232518</td>\n",
              "      <td>0.232440</td>\n",
              "      <td>0.235256</td>\n",
              "      <td>0.240111</td>\n",
              "      <td>0.245109</td>\n",
              "      <td>0.248335</td>\n",
              "      <td>0.247105</td>\n",
              "      <td>0.243342</td>\n",
              "      <td>0.242165</td>\n",
              "      <td>0.241029</td>\n",
              "      <td>0.235705</td>\n",
              "      <td>0.230195</td>\n",
              "      <td>0.228879</td>\n",
              "      <td>0.231738</td>\n",
              "      <td>0.235806</td>\n",
              "      <td>0.237096</td>\n",
              "      <td>0.235260</td>\n",
              "      <td>0.233820</td>\n",
              "      <td>0.235355</td>\n",
              "      <td>0.236107</td>\n",
              "      <td>0.232505</td>\n",
              "      <td>0.229418</td>\n",
              "      <td>0.229454</td>\n",
              "      <td>0.227992</td>\n",
              "      <td>0.224687</td>\n",
              "      <td>0.223451</td>\n",
              "      <td>0.224735</td>\n",
              "      <td>0.227557</td>\n",
              "      <td>0.233737</td>\n",
              "      <td>0.241304</td>\n",
              "      <td>0.243359</td>\n",
              "      <td>...</td>\n",
              "      <td>1.641270</td>\n",
              "      <td>1.641635</td>\n",
              "      <td>1.641760</td>\n",
              "      <td>1.641625</td>\n",
              "      <td>1.640929</td>\n",
              "      <td>1.639455</td>\n",
              "      <td>1.637257</td>\n",
              "      <td>1.634512</td>\n",
              "      <td>1.631092</td>\n",
              "      <td>1.626688</td>\n",
              "      <td>1.621387</td>\n",
              "      <td>1.615671</td>\n",
              "      <td>1.609609</td>\n",
              "      <td>1.602812</td>\n",
              "      <td>1.595395</td>\n",
              "      <td>1.588021</td>\n",
              "      <td>1.581357</td>\n",
              "      <td>1.575661</td>\n",
              "      <td>1.570702</td>\n",
              "      <td>1.566095</td>\n",
              "      <td>-0.572226</td>\n",
              "      <td>-0.623242</td>\n",
              "      <td>-0.691976</td>\n",
              "      <td>-0.213203</td>\n",
              "      <td>0.533953</td>\n",
              "      <td>0.698859</td>\n",
              "      <td>-0.413636</td>\n",
              "      <td>-0.092569</td>\n",
              "      <td>-0.698528</td>\n",
              "      <td>-0.506340</td>\n",
              "      <td>-0.661642</td>\n",
              "      <td>-0.638464</td>\n",
              "      <td>0.276786</td>\n",
              "      <td>0.563194</td>\n",
              "      <td>0.746303</td>\n",
              "      <td>0.006442</td>\n",
              "      <td>-0.014524</td>\n",
              "      <td>-0.028543</td>\n",
              "      <td>0.080414</td>\n",
              "      <td>-0.012646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.114439</td>\n",
              "      <td>0.114804</td>\n",
              "      <td>0.115288</td>\n",
              "      <td>0.115075</td>\n",
              "      <td>0.114185</td>\n",
              "      <td>0.113603</td>\n",
              "      <td>0.113974</td>\n",
              "      <td>0.114723</td>\n",
              "      <td>0.115031</td>\n",
              "      <td>0.115021</td>\n",
              "      <td>0.114974</td>\n",
              "      <td>0.114913</td>\n",
              "      <td>0.114762</td>\n",
              "      <td>0.114572</td>\n",
              "      <td>0.114477</td>\n",
              "      <td>0.114519</td>\n",
              "      <td>0.114623</td>\n",
              "      <td>0.114566</td>\n",
              "      <td>0.114427</td>\n",
              "      <td>0.114439</td>\n",
              "      <td>0.114467</td>\n",
              "      <td>0.114466</td>\n",
              "      <td>0.114531</td>\n",
              "      <td>0.114583</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.114651</td>\n",
              "      <td>0.114737</td>\n",
              "      <td>0.114796</td>\n",
              "      <td>0.114726</td>\n",
              "      <td>0.114625</td>\n",
              "      <td>0.114665</td>\n",
              "      <td>0.114704</td>\n",
              "      <td>0.114649</td>\n",
              "      <td>0.114636</td>\n",
              "      <td>0.114734</td>\n",
              "      <td>0.114871</td>\n",
              "      <td>0.115001</td>\n",
              "      <td>0.114959</td>\n",
              "      <td>0.114802</td>\n",
              "      <td>0.114902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.241704</td>\n",
              "      <td>0.239736</td>\n",
              "      <td>0.237488</td>\n",
              "      <td>0.234900</td>\n",
              "      <td>0.231795</td>\n",
              "      <td>0.227959</td>\n",
              "      <td>0.223447</td>\n",
              "      <td>0.218600</td>\n",
              "      <td>0.213626</td>\n",
              "      <td>0.208479</td>\n",
              "      <td>0.203323</td>\n",
              "      <td>0.198371</td>\n",
              "      <td>0.193652</td>\n",
              "      <td>0.189194</td>\n",
              "      <td>0.185172</td>\n",
              "      <td>0.181657</td>\n",
              "      <td>0.178563</td>\n",
              "      <td>0.175859</td>\n",
              "      <td>0.173415</td>\n",
              "      <td>0.171090</td>\n",
              "      <td>0.235748</td>\n",
              "      <td>0.239737</td>\n",
              "      <td>0.281652</td>\n",
              "      <td>0.661576</td>\n",
              "      <td>1.396132</td>\n",
              "      <td>0.683280</td>\n",
              "      <td>0.686773</td>\n",
              "      <td>0.861165</td>\n",
              "      <td>0.272107</td>\n",
              "      <td>0.329785</td>\n",
              "      <td>0.365572</td>\n",
              "      <td>0.326460</td>\n",
              "      <td>1.074667</td>\n",
              "      <td>0.649622</td>\n",
              "      <td>0.825242</td>\n",
              "      <td>1.070541</td>\n",
              "      <td>0.995469</td>\n",
              "      <td>0.920224</td>\n",
              "      <td>1.141989</td>\n",
              "      <td>0.988520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.042260</td>\n",
              "      <td>-0.048559</td>\n",
              "      <td>-0.055518</td>\n",
              "      <td>-0.052353</td>\n",
              "      <td>-0.040608</td>\n",
              "      <td>-0.034516</td>\n",
              "      <td>-0.042619</td>\n",
              "      <td>-0.053856</td>\n",
              "      <td>-0.057699</td>\n",
              "      <td>-0.058482</td>\n",
              "      <td>-0.058516</td>\n",
              "      <td>-0.055382</td>\n",
              "      <td>-0.050047</td>\n",
              "      <td>-0.044445</td>\n",
              "      <td>-0.040496</td>\n",
              "      <td>-0.041609</td>\n",
              "      <td>-0.046195</td>\n",
              "      <td>-0.047614</td>\n",
              "      <td>-0.048243</td>\n",
              "      <td>-0.053698</td>\n",
              "      <td>-0.059410</td>\n",
              "      <td>-0.060471</td>\n",
              "      <td>-0.057686</td>\n",
              "      <td>-0.053916</td>\n",
              "      <td>-0.052795</td>\n",
              "      <td>-0.054845</td>\n",
              "      <td>-0.056737</td>\n",
              "      <td>-0.055440</td>\n",
              "      <td>-0.054183</td>\n",
              "      <td>-0.057408</td>\n",
              "      <td>-0.061050</td>\n",
              "      <td>-0.061714</td>\n",
              "      <td>-0.063246</td>\n",
              "      <td>-0.066335</td>\n",
              "      <td>-0.067661</td>\n",
              "      <td>-0.066865</td>\n",
              "      <td>-0.064690</td>\n",
              "      <td>-0.058404</td>\n",
              "      <td>-0.049749</td>\n",
              "      <td>-0.047555</td>\n",
              "      <td>...</td>\n",
              "      <td>0.857501</td>\n",
              "      <td>0.860989</td>\n",
              "      <td>0.864792</td>\n",
              "      <td>0.868868</td>\n",
              "      <td>0.873789</td>\n",
              "      <td>0.880374</td>\n",
              "      <td>0.888695</td>\n",
              "      <td>0.898290</td>\n",
              "      <td>0.908381</td>\n",
              "      <td>0.918089</td>\n",
              "      <td>0.927082</td>\n",
              "      <td>0.935663</td>\n",
              "      <td>0.944329</td>\n",
              "      <td>0.953882</td>\n",
              "      <td>0.964296</td>\n",
              "      <td>0.975861</td>\n",
              "      <td>0.987868</td>\n",
              "      <td>0.999643</td>\n",
              "      <td>1.012760</td>\n",
              "      <td>1.023430</td>\n",
              "      <td>-1.007246</td>\n",
              "      <td>-0.972727</td>\n",
              "      <td>-1.175812</td>\n",
              "      <td>-0.945472</td>\n",
              "      <td>-1.326726</td>\n",
              "      <td>-0.881924</td>\n",
              "      <td>-1.906958</td>\n",
              "      <td>-2.720276</td>\n",
              "      <td>-1.127273</td>\n",
              "      <td>-1.642857</td>\n",
              "      <td>-1.265010</td>\n",
              "      <td>-1.115423</td>\n",
              "      <td>-0.639823</td>\n",
              "      <td>-0.670742</td>\n",
              "      <td>-0.862741</td>\n",
              "      <td>-0.535828</td>\n",
              "      <td>-0.418309</td>\n",
              "      <td>-1.886946</td>\n",
              "      <td>-0.857863</td>\n",
              "      <td>-1.493378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.171156</td>\n",
              "      <td>0.166020</td>\n",
              "      <td>0.161043</td>\n",
              "      <td>0.164470</td>\n",
              "      <td>0.173065</td>\n",
              "      <td>0.175476</td>\n",
              "      <td>0.169058</td>\n",
              "      <td>0.161094</td>\n",
              "      <td>0.159238</td>\n",
              "      <td>0.158868</td>\n",
              "      <td>0.158728</td>\n",
              "      <td>0.160848</td>\n",
              "      <td>0.165475</td>\n",
              "      <td>0.169978</td>\n",
              "      <td>0.173684</td>\n",
              "      <td>0.172663</td>\n",
              "      <td>0.168472</td>\n",
              "      <td>0.167478</td>\n",
              "      <td>0.166162</td>\n",
              "      <td>0.162227</td>\n",
              "      <td>0.156418</td>\n",
              "      <td>0.154731</td>\n",
              "      <td>0.157410</td>\n",
              "      <td>0.161752</td>\n",
              "      <td>0.162982</td>\n",
              "      <td>0.161502</td>\n",
              "      <td>0.159786</td>\n",
              "      <td>0.161439</td>\n",
              "      <td>0.162058</td>\n",
              "      <td>0.158072</td>\n",
              "      <td>0.155576</td>\n",
              "      <td>0.155380</td>\n",
              "      <td>0.153694</td>\n",
              "      <td>0.149859</td>\n",
              "      <td>0.149006</td>\n",
              "      <td>0.149846</td>\n",
              "      <td>0.151918</td>\n",
              "      <td>0.158941</td>\n",
              "      <td>0.165572</td>\n",
              "      <td>0.167740</td>\n",
              "      <td>...</td>\n",
              "      <td>1.494920</td>\n",
              "      <td>1.497070</td>\n",
              "      <td>1.498480</td>\n",
              "      <td>1.499250</td>\n",
              "      <td>1.500720</td>\n",
              "      <td>1.499550</td>\n",
              "      <td>1.499970</td>\n",
              "      <td>1.497650</td>\n",
              "      <td>1.495140</td>\n",
              "      <td>1.493280</td>\n",
              "      <td>1.488950</td>\n",
              "      <td>1.486420</td>\n",
              "      <td>1.484300</td>\n",
              "      <td>1.478590</td>\n",
              "      <td>1.469110</td>\n",
              "      <td>1.463620</td>\n",
              "      <td>1.457130</td>\n",
              "      <td>1.455110</td>\n",
              "      <td>1.447870</td>\n",
              "      <td>1.443850</td>\n",
              "      <td>-0.739130</td>\n",
              "      <td>-0.781818</td>\n",
              "      <td>-0.895857</td>\n",
              "      <td>-0.548063</td>\n",
              "      <td>-0.812249</td>\n",
              "      <td>0.199708</td>\n",
              "      <td>-0.906601</td>\n",
              "      <td>-0.622874</td>\n",
              "      <td>-0.900364</td>\n",
              "      <td>-0.753247</td>\n",
              "      <td>-0.917184</td>\n",
              "      <td>-0.881048</td>\n",
              "      <td>-0.452939</td>\n",
              "      <td>0.190708</td>\n",
              "      <td>0.056843</td>\n",
              "      <td>-0.451077</td>\n",
              "      <td>-0.345681</td>\n",
              "      <td>-0.717841</td>\n",
              "      <td>-0.615639</td>\n",
              "      <td>-0.899649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.252899</td>\n",
              "      <td>0.247918</td>\n",
              "      <td>0.244594</td>\n",
              "      <td>0.247920</td>\n",
              "      <td>0.255784</td>\n",
              "      <td>0.258029</td>\n",
              "      <td>0.251061</td>\n",
              "      <td>0.243775</td>\n",
              "      <td>0.241991</td>\n",
              "      <td>0.241599</td>\n",
              "      <td>0.241637</td>\n",
              "      <td>0.244274</td>\n",
              "      <td>0.248483</td>\n",
              "      <td>0.252460</td>\n",
              "      <td>0.255168</td>\n",
              "      <td>0.253936</td>\n",
              "      <td>0.250830</td>\n",
              "      <td>0.250147</td>\n",
              "      <td>0.248605</td>\n",
              "      <td>0.244203</td>\n",
              "      <td>0.238798</td>\n",
              "      <td>0.237834</td>\n",
              "      <td>0.240731</td>\n",
              "      <td>0.244748</td>\n",
              "      <td>0.245389</td>\n",
              "      <td>0.243736</td>\n",
              "      <td>0.242696</td>\n",
              "      <td>0.244079</td>\n",
              "      <td>0.244454</td>\n",
              "      <td>0.241205</td>\n",
              "      <td>0.238221</td>\n",
              "      <td>0.238287</td>\n",
              "      <td>0.236895</td>\n",
              "      <td>0.233179</td>\n",
              "      <td>0.232356</td>\n",
              "      <td>0.233557</td>\n",
              "      <td>0.236542</td>\n",
              "      <td>0.242229</td>\n",
              "      <td>0.249218</td>\n",
              "      <td>0.250050</td>\n",
              "      <td>...</td>\n",
              "      <td>1.672930</td>\n",
              "      <td>1.671620</td>\n",
              "      <td>1.672680</td>\n",
              "      <td>1.671130</td>\n",
              "      <td>1.668720</td>\n",
              "      <td>1.664840</td>\n",
              "      <td>1.658420</td>\n",
              "      <td>1.655520</td>\n",
              "      <td>1.650330</td>\n",
              "      <td>1.641390</td>\n",
              "      <td>1.632500</td>\n",
              "      <td>1.623430</td>\n",
              "      <td>1.615780</td>\n",
              "      <td>1.605710</td>\n",
              "      <td>1.599760</td>\n",
              "      <td>1.589820</td>\n",
              "      <td>1.577530</td>\n",
              "      <td>1.575070</td>\n",
              "      <td>1.572230</td>\n",
              "      <td>1.565840</td>\n",
              "      <td>-0.608696</td>\n",
              "      <td>-0.681818</td>\n",
              "      <td>-0.761478</td>\n",
              "      <td>-0.344714</td>\n",
              "      <td>0.867038</td>\n",
              "      <td>0.659621</td>\n",
              "      <td>-0.476813</td>\n",
              "      <td>-0.019511</td>\n",
              "      <td>-0.754182</td>\n",
              "      <td>-0.531540</td>\n",
              "      <td>-0.753623</td>\n",
              "      <td>-0.740423</td>\n",
              "      <td>-0.130139</td>\n",
              "      <td>0.316667</td>\n",
              "      <td>0.729111</td>\n",
              "      <td>-0.348682</td>\n",
              "      <td>-0.269595</td>\n",
              "      <td>-0.175376</td>\n",
              "      <td>-0.349974</td>\n",
              "      <td>-0.134651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.315508</td>\n",
              "      <td>0.310354</td>\n",
              "      <td>0.304742</td>\n",
              "      <td>0.309540</td>\n",
              "      <td>0.317786</td>\n",
              "      <td>0.320834</td>\n",
              "      <td>0.314091</td>\n",
              "      <td>0.304301</td>\n",
              "      <td>0.303235</td>\n",
              "      <td>0.302438</td>\n",
              "      <td>0.302016</td>\n",
              "      <td>0.304905</td>\n",
              "      <td>0.310274</td>\n",
              "      <td>0.314133</td>\n",
              "      <td>0.318184</td>\n",
              "      <td>0.316865</td>\n",
              "      <td>0.312544</td>\n",
              "      <td>0.311166</td>\n",
              "      <td>0.310503</td>\n",
              "      <td>0.305297</td>\n",
              "      <td>0.299883</td>\n",
              "      <td>0.298926</td>\n",
              "      <td>0.300635</td>\n",
              "      <td>0.305245</td>\n",
              "      <td>0.306757</td>\n",
              "      <td>0.304770</td>\n",
              "      <td>0.303451</td>\n",
              "      <td>0.304752</td>\n",
              "      <td>0.306371</td>\n",
              "      <td>0.301623</td>\n",
              "      <td>0.299044</td>\n",
              "      <td>0.299347</td>\n",
              "      <td>0.298507</td>\n",
              "      <td>0.295365</td>\n",
              "      <td>0.294306</td>\n",
              "      <td>0.295783</td>\n",
              "      <td>0.298522</td>\n",
              "      <td>0.304226</td>\n",
              "      <td>0.311147</td>\n",
              "      <td>0.313506</td>\n",
              "      <td>...</td>\n",
              "      <td>1.810820</td>\n",
              "      <td>1.810600</td>\n",
              "      <td>1.807070</td>\n",
              "      <td>1.805470</td>\n",
              "      <td>1.802010</td>\n",
              "      <td>1.797390</td>\n",
              "      <td>1.794090</td>\n",
              "      <td>1.787140</td>\n",
              "      <td>1.781280</td>\n",
              "      <td>1.773530</td>\n",
              "      <td>1.765060</td>\n",
              "      <td>1.757340</td>\n",
              "      <td>1.750270</td>\n",
              "      <td>1.741990</td>\n",
              "      <td>1.732700</td>\n",
              "      <td>1.722190</td>\n",
              "      <td>1.711970</td>\n",
              "      <td>1.704110</td>\n",
              "      <td>1.694240</td>\n",
              "      <td>1.687500</td>\n",
              "      <td>-0.463768</td>\n",
              "      <td>-0.563636</td>\n",
              "      <td>-0.604703</td>\n",
              "      <td>-0.104917</td>\n",
              "      <td>1.292428</td>\n",
              "      <td>1.129738</td>\n",
              "      <td>0.080083</td>\n",
              "      <td>0.631670</td>\n",
              "      <td>-0.594909</td>\n",
              "      <td>-0.269944</td>\n",
              "      <td>-0.445135</td>\n",
              "      <td>-0.432460</td>\n",
              "      <td>0.532450</td>\n",
              "      <td>0.955935</td>\n",
              "      <td>1.414215</td>\n",
              "      <td>-0.042654</td>\n",
              "      <td>-0.089755</td>\n",
              "      <td>0.376442</td>\n",
              "      <td>0.275121</td>\n",
              "      <td>0.786391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.730793</td>\n",
              "      <td>0.725493</td>\n",
              "      <td>0.720711</td>\n",
              "      <td>0.723293</td>\n",
              "      <td>0.731205</td>\n",
              "      <td>0.733872</td>\n",
              "      <td>0.726075</td>\n",
              "      <td>0.717652</td>\n",
              "      <td>0.716443</td>\n",
              "      <td>0.716307</td>\n",
              "      <td>0.715540</td>\n",
              "      <td>0.718900</td>\n",
              "      <td>0.725116</td>\n",
              "      <td>0.731466</td>\n",
              "      <td>0.735929</td>\n",
              "      <td>0.734779</td>\n",
              "      <td>0.729934</td>\n",
              "      <td>0.727856</td>\n",
              "      <td>0.726349</td>\n",
              "      <td>0.720007</td>\n",
              "      <td>0.712521</td>\n",
              "      <td>0.710120</td>\n",
              "      <td>0.714294</td>\n",
              "      <td>0.720478</td>\n",
              "      <td>0.722384</td>\n",
              "      <td>0.719997</td>\n",
              "      <td>0.718446</td>\n",
              "      <td>0.721279</td>\n",
              "      <td>0.722782</td>\n",
              "      <td>0.717108</td>\n",
              "      <td>0.711940</td>\n",
              "      <td>0.711861</td>\n",
              "      <td>0.709282</td>\n",
              "      <td>0.703568</td>\n",
              "      <td>0.701358</td>\n",
              "      <td>0.703629</td>\n",
              "      <td>0.708326</td>\n",
              "      <td>0.717285</td>\n",
              "      <td>0.728495</td>\n",
              "      <td>0.732891</td>\n",
              "      <td>...</td>\n",
              "      <td>2.220350</td>\n",
              "      <td>2.210410</td>\n",
              "      <td>2.195790</td>\n",
              "      <td>2.183360</td>\n",
              "      <td>2.175200</td>\n",
              "      <td>2.160680</td>\n",
              "      <td>2.145220</td>\n",
              "      <td>2.125040</td>\n",
              "      <td>2.110090</td>\n",
              "      <td>2.094960</td>\n",
              "      <td>2.074840</td>\n",
              "      <td>2.069870</td>\n",
              "      <td>2.063750</td>\n",
              "      <td>2.060970</td>\n",
              "      <td>2.062620</td>\n",
              "      <td>2.058860</td>\n",
              "      <td>2.044370</td>\n",
              "      <td>2.028590</td>\n",
              "      <td>2.021880</td>\n",
              "      <td>2.014480</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.223964</td>\n",
              "      <td>3.596526</td>\n",
              "      <td>4.891537</td>\n",
              "      <td>2.645044</td>\n",
              "      <td>1.320112</td>\n",
              "      <td>1.519218</td>\n",
              "      <td>0.285818</td>\n",
              "      <td>0.335807</td>\n",
              "      <td>0.366460</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>5.612300</td>\n",
              "      <td>2.161892</td>\n",
              "      <td>2.976315</td>\n",
              "      <td>9.645815</td>\n",
              "      <td>13.266841</td>\n",
              "      <td>3.416117</td>\n",
              "      <td>7.619989</td>\n",
              "      <td>2.251685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 3598 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          m7497.96     m7496.04  ...          SOC         Sand\n",
              "count  1157.000000  1157.000000  ...  1157.000000  1157.000000\n",
              "mean      0.245666     0.240454  ...     0.080414    -0.012646\n",
              "std       0.114439     0.114804  ...     1.141989     0.988520\n",
              "min      -0.042260    -0.048559  ...    -0.857863    -1.493378\n",
              "25%       0.171156     0.166020  ...    -0.615639    -0.899649\n",
              "50%       0.252899     0.247918  ...    -0.349974    -0.134651\n",
              "75%       0.315508     0.310354  ...     0.275121     0.786391\n",
              "max       0.730793     0.725493  ...     7.619989     2.251685\n",
              "\n",
              "[8 rows x 3598 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcdcuFvXDyno"
      },
      "source": [
        "Show if any missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m4togw6D2Jl",
        "outputId": "187fbfdf-c0d2-4c1a-859d-d8fe92263293"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PIDN        0\n",
              "m7497.96    0\n",
              "m7496.04    0\n",
              "m7494.11    0\n",
              "m7492.18    0\n",
              "           ..\n",
              "Ca          0\n",
              "P           0\n",
              "pH          0\n",
              "SOC         0\n",
              "Sand        0\n",
              "Length: 3600, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6glW7NQEdqF"
      },
      "source": [
        "###Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlA_2nBKE57j"
      },
      "source": [
        "Count Plot for categorical variables/features (depth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "OoA1owlZFFZf",
        "outputId": "bc18fcc0-4c37-48a7-d56d-a6c1ca118a5c"
      },
      "source": [
        "fig = plt.figure(figsize=(20, 15))\n",
        "grid = fig.add_gridspec(3, 3)\n",
        "grid.update(wspace = 0.5, hspace = 0.25)\n",
        "#axis_0 = fig.add_subplot(grid[0,0])\n",
        "axis_1 = fig.add_subplot(grid[0, 1])\n",
        "bg_color = \"#ffffff\"\n",
        "#color_palette = [\"#800000\",\"#8000ff\",\"#6aac90\",\"#5833ff\",\"#da8829\"]\n",
        "#axis_0.set_facecolor(bg_color)\n",
        "axis_1.set_facecolor(bg_color)\n",
        "\n",
        "#Depth count --- Run, then Go over the code to add whatever is needed or wanted\n",
        "axis_1.text(0.0, 400, 'Sample Depth', fontsize = 15, fontweight = 'bold', fontfamily = 'serif', color = '#000')\n",
        "axis_1.grid(axis = 'y')\n",
        "sns.countplot(ax = axis_1, data = df, x = 'Depth')\n",
        "''', palette = color_palette'''\n",
        "axis_1.set_xlabel(\"\")\n",
        "axis_1.set_ylabel(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZW6BkPPFt7I"
      },
      "source": [
        "---\n",
        "---\n",
        "#**Data Modelling**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JslPPn8XFwMQ"
      },
      "source": [
        "#To split data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For use in building models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "#More for Neural Networks \n",
        "from tensorflow import keras\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Performance metrics of models\n",
        "from sklearn import metrics #accuracy_score, classification_report, roc_curve\n",
        "\n",
        "# Model cross validation\n",
        "#from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3JuDBHGuPd8"
      },
      "source": [
        "##Calcium Content\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-GnAsXjIvoK"
      },
      "source": [
        "Defining predictors and output data \n",
        "\n",
        "(Features have been mean-centered and scaled)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hesW7ZcHIvUJ",
        "outputId": "1858b9bd-b26c-46d4-89a4-0db39d295f38"
      },
      "source": [
        "#Calcium content\n",
        "\n",
        "#Creating a (second) copy of dataset\n",
        "data2 = df\n",
        "\n",
        "categorical = ['Depth']\n",
        "continuous = data2.drop(['PIDN', 'Depth'], axis = 1)\n",
        "\n",
        "#Encoding categorical parameter (Depth)\n",
        "data2 = pd.get_dummies(data2, columns = categorical, drop_first = True)\n",
        "\n",
        "#Defining predictors and target\n",
        "X = data2.drop(['PIDN',\"Ca\",\"P\",\"pH\",\"SOC\",\"Sand\"], axis = 1)\n",
        "y = data2[['Ca']]\n",
        "\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "#print(\"Preview predictors:\")\n",
        "#X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1157, 3594)\n",
            "(1157, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LuattXSqa-P"
      },
      "source": [
        "Split into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vd4O9t-qfwv",
        "outputId": "3fa19aec-7dfb-434d-d0cd-5e506630eef9"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(925, 3594) (925, 1) (232, 3594) (232, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8dYpEw36jT4"
      },
      "source": [
        "####**Support Vector Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6d_8Mkp6nra",
        "outputId": "b6db1d04-c0e3-4d64-f13b-eef160eadf9f"
      },
      "source": [
        "#SVR Model Fitting\n",
        "regressor = SVR(kernel = 'rbf')#Radial Basis Function\n",
        "\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "#Predicting Ca values\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "#Prediction Accuracy\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "#New kernel and gamma values \n",
        "#based on the performance of \n",
        "#Hyperparameter Tuning/Testing\n",
        "poly_regressor = SVR(kernel = 'poly', gamma = 0.01)#the polynomial kernel has a parameter 'degree' (default = 3). Consider testing others\n",
        "\n",
        "poly_regressor.fit(X_train, y_train)\n",
        "\n",
        "#Predicting Ca values\n",
        "y_pred = poly_regressor.predict(X_test)\n",
        "\n",
        "#Prediction Accuracy\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))\n",
        "\n",
        "'''The hyperparameter tuning exercise below (next cells) provided considerably \n",
        "higher accuracy metrics as shown in the results, using the 'poly' \n",
        "(polynomial) kernel function and a gamma value = 0.01.\n",
        "This model obviously took more time to train (only seconds, \n",
        "but clearly more than the default 'rbf' model used above).'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RÂ²: 0.5956015195308084\n",
            "Variance Regression Score: 0.6046502278776587\n",
            "max_error: 5.6680331524835825\n",
            "mean_absolute_error: 0.23952150994934113\n",
            "mean_squared_error: 0.4335912548830192\n",
            "Root mean squared error:  0.6584764649423844\n",
            "median_absolute_error: 0.08561159941127963\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RÂ²: 0.8293971667600887\n",
            "Variance Regression Score: 0.829514539787877\n",
            "max_error: 4.103939317350771\n",
            "mean_absolute_error: 0.1697144618039598\n",
            "mean_squared_error: 0.1829183345725431\n",
            "Root mean squared error:  0.4276895305856143\n",
            "median_absolute_error: 0.07861436349393586\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The hyperparameter tuning exercise below (next cells) provided considerably \\nhigher accuracy metrics as shown in the results, using the 'poly' \\n(polynomial) kernel function and a gamma value = 0.01.\\nThis model obviously took more time to train (only seconds, \\nbut clearly more than the default 'rbf' model used above).\""
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IStCFkNXe_m"
      },
      "source": [
        "Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Cw_sMhYdvR"
      },
      "source": [
        "#This probably test different parameters to find the most accurate combination\n",
        "regressor = SVR()\n",
        "\n",
        "#Grid:\n",
        "parameters = {\n",
        "    \"kernel\": ['rbf', 'poly', 'sigmoid', 'linear'], \n",
        "    \"gamma\": [0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]\n",
        "    }#(for gamma: '1e-4,1e-2,0.0001' ?)\n",
        "\n",
        "#Test parameters using the 'GridSearchCV' object\n",
        "test = GridSearchCV(regressor, parameters)\n",
        "\n",
        "\n",
        "#Fitting the model\n",
        "test.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#Parameters performance scores\n",
        "print(\"Best parameters: \", test.best_params_)\n",
        "print(\"Best score: \", test.best_score_)\n",
        "\n",
        "#Predicting Ca values\n",
        "y_pred = test.predict(X_test)\n",
        "\n",
        "#The resulting accuracy of the test:\n",
        "print(\"The prediction accuracy of the SVR model after testing/tuning parameters:\")\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCUXm1SdtUXy"
      },
      "source": [
        "####**Random Forest Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOALOWwut7JN",
        "outputId": "4206a282-97fd-44cd-d16a-89d2622b54b7"
      },
      "source": [
        "#Defining the object/model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "#Training/Fitting the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "#Predicting values of Ca content\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "\n",
        "#The resulting accuracy of the Random Forest Model:\n",
        "print(\"The prediction accuracy of the Random Forest model:\")\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))\n",
        "\n",
        "'''Based on the initial performance of this model, \n",
        "hyperparameter tuning (as in the case of the SVM model) may be worth trying'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \"\"\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The prediction accuracy of the Logistic Regression model:\n",
            "RÂ²: 0.8292456787757203\n",
            "Variance Regression Score: 0.8298928513539446\n",
            "max_error: 3.3392115928176627\n",
            "mean_absolute_error: 0.19003020238894922\n",
            "mean_squared_error: 0.1830807581928439\n",
            "Root mean squared error:  0.4278793734136338\n",
            "median_absolute_error: 0.0779300650048044\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Based on the initial performance of this model, \\nhyperparameter tuning (as in the case of the SVM model) may be worth trying'"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4V9r6wmxq5V"
      },
      "source": [
        "####**Attempting Gradient boosted trees (XGBoost)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW1KbcnUD3M1"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4u1-Cx45GKM"
      },
      "source": [
        "#Define a custom loss\n",
        "\n",
        "from typing import Tuple\n",
        "#!pip install dmatrix2np\n",
        "#import dmatrix2np as dm2np\n",
        "#print(dir(dmatrix2np))\n",
        "\n",
        "#{Pasting from here}\n",
        "def gradient_se(y_pred: np.ndarray, y_train: xgb.DMatrix) -> np.ndarray: \n",
        "  #Compute the gradient squared error.\n",
        "  #dm2np.dmatrix_to_numpy(dtrain)\n",
        "  y_pred = xgb.DMatrix(X_test, label = y_pred)\n",
        "  return 2*(y_pred - y_train)\n",
        "\n",
        "def hessian_se(y_pred: np.ndarray, y_train: xgb.DMatrix) -> np.ndarray: \n",
        "  #Compute the hessian for squared error\n",
        "  return 0*y_train + 2\n",
        "\n",
        "#Possible option for function: y_pred: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]: \n",
        "\n",
        "def custom_mse(y_pred: np.ndarray, y_train: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
        "  #squared error objective. A simplified version of MSE used as\n",
        "  #objective function.\n",
        "  grad = gradient_se(y_pred, y_train)\n",
        "  hess = hessian_se(y_pred, y_train) #y_pred, dtest\n",
        "  return grad, hess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEPzEflcyGjF",
        "outputId": "85fff224-cd7e-4187-90ad-d02b180404e9"
      },
      "source": [
        "#Loading training data into DMatrices (required for XGBoost API)\n",
        "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
        "dtest = xgb.DMatrix(X_test, label = y_test)\n",
        "\n",
        "\n",
        "#Define parameters yet to be tuned (default::- to experiment with hp tuning)\n",
        "params = {\n",
        "    'max_depth':6,\n",
        "    'min_child_weight':1,\n",
        "    'eta':0.3,\n",
        "    'subsample':1,\n",
        "    'colsample_bytree':1,\n",
        "    #another param\n",
        "    'objective':'reg:squarederror', # 'reg:linear' deprecated\n",
        "    #'obj': 'custom_mse'\n",
        "}\n",
        "\n",
        "#Other parameters\n",
        "params['eval_metric'] = \"mae\"\n",
        "num_boost_round = 999 #maximum number of boosting rounds\n",
        "\n",
        "#model\n",
        "xgbmodel = xgb.train(\n",
        "    params, \n",
        "    dtrain,\n",
        "    num_boost_round = num_boost_round, \n",
        "    evals = [(dtest, \"Test\")],\n",
        "    #obj = custom_mse,\n",
        "    early_stopping_rounds = 10 #number of rounds without improvement before stopping\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTest-mae:0.654159\n",
            "Will train until Test-mae hasn't improved in 10 rounds.\n",
            "[1]\tTest-mae:0.509381\n",
            "[2]\tTest-mae:0.403654\n",
            "[3]\tTest-mae:0.337829\n",
            "[4]\tTest-mae:0.298321\n",
            "[5]\tTest-mae:0.269235\n",
            "[6]\tTest-mae:0.251828\n",
            "[7]\tTest-mae:0.237538\n",
            "[8]\tTest-mae:0.228266\n",
            "[9]\tTest-mae:0.22062\n",
            "[10]\tTest-mae:0.215536\n",
            "[11]\tTest-mae:0.210267\n",
            "[12]\tTest-mae:0.206822\n",
            "[13]\tTest-mae:0.203927\n",
            "[14]\tTest-mae:0.199921\n",
            "[15]\tTest-mae:0.198342\n",
            "[16]\tTest-mae:0.196857\n",
            "[17]\tTest-mae:0.194788\n",
            "[18]\tTest-mae:0.19379\n",
            "[19]\tTest-mae:0.192712\n",
            "[20]\tTest-mae:0.192482\n",
            "[21]\tTest-mae:0.19156\n",
            "[22]\tTest-mae:0.190732\n",
            "[23]\tTest-mae:0.189458\n",
            "[24]\tTest-mae:0.188841\n",
            "[25]\tTest-mae:0.188348\n",
            "[26]\tTest-mae:0.187628\n",
            "[27]\tTest-mae:0.187522\n",
            "[28]\tTest-mae:0.187152\n",
            "[29]\tTest-mae:0.186993\n",
            "[30]\tTest-mae:0.186609\n",
            "[31]\tTest-mae:0.185666\n",
            "[32]\tTest-mae:0.185612\n",
            "[33]\tTest-mae:0.185459\n",
            "[34]\tTest-mae:0.185314\n",
            "[35]\tTest-mae:0.185261\n",
            "[36]\tTest-mae:0.185278\n",
            "[37]\tTest-mae:0.185181\n",
            "[38]\tTest-mae:0.184982\n",
            "[39]\tTest-mae:0.184949\n",
            "[40]\tTest-mae:0.184632\n",
            "[41]\tTest-mae:0.184526\n",
            "[42]\tTest-mae:0.184522\n",
            "[43]\tTest-mae:0.184297\n",
            "[44]\tTest-mae:0.184309\n",
            "[45]\tTest-mae:0.184395\n",
            "[46]\tTest-mae:0.184366\n",
            "[47]\tTest-mae:0.18419\n",
            "[48]\tTest-mae:0.184254\n",
            "[49]\tTest-mae:0.18422\n",
            "[50]\tTest-mae:0.184293\n",
            "[51]\tTest-mae:0.184272\n",
            "[52]\tTest-mae:0.184151\n",
            "[53]\tTest-mae:0.184011\n",
            "[54]\tTest-mae:0.184047\n",
            "[55]\tTest-mae:0.184024\n",
            "[56]\tTest-mae:0.184042\n",
            "[57]\tTest-mae:0.184051\n",
            "[58]\tTest-mae:0.184038\n",
            "[59]\tTest-mae:0.18391\n",
            "[60]\tTest-mae:0.183824\n",
            "[61]\tTest-mae:0.183804\n",
            "[62]\tTest-mae:0.183769\n",
            "[63]\tTest-mae:0.183758\n",
            "[64]\tTest-mae:0.183703\n",
            "[65]\tTest-mae:0.183762\n",
            "[66]\tTest-mae:0.183746\n",
            "[67]\tTest-mae:0.183724\n",
            "[68]\tTest-mae:0.183742\n",
            "[69]\tTest-mae:0.183752\n",
            "[70]\tTest-mae:0.183722\n",
            "[71]\tTest-mae:0.183731\n",
            "[72]\tTest-mae:0.183706\n",
            "[73]\tTest-mae:0.183708\n",
            "[74]\tTest-mae:0.183692\n",
            "[75]\tTest-mae:0.18368\n",
            "[76]\tTest-mae:0.183698\n",
            "[77]\tTest-mae:0.183703\n",
            "[78]\tTest-mae:0.183738\n",
            "[79]\tTest-mae:0.183732\n",
            "[80]\tTest-mae:0.183732\n",
            "[81]\tTest-mae:0.183742\n",
            "[82]\tTest-mae:0.183747\n",
            "[83]\tTest-mae:0.183718\n",
            "[84]\tTest-mae:0.183736\n",
            "[85]\tTest-mae:0.183733\n",
            "Stopping. Best iteration:\n",
            "[75]\tTest-mae:0.18368\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwaYaIV2D_QG"
      },
      "source": [
        "Experiment cross-validation w/ default parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw4R35M07atw"
      },
      "source": [
        "#see cross-validation score at current parameters\n",
        "crossval_results = xgb.cv(\n",
        "    params, \n",
        "    dtrain,\n",
        "    num_boost_round = num_boost_round, \n",
        "    seed = 42, \n",
        "    nfold = 5, \n",
        "    early_stopping_rounds = 10\n",
        ")\n",
        "\n",
        "#Log the results\n",
        "'''xgb_Gridcv.best_estimator_.evals_result()\n",
        "log_result = [(dtest, 'eval')]'''\n",
        "\n",
        "''' The 'cv' parameter stands for 'Cross-validation'. The number of \n",
        "'folds' to use in cross-validation will be passed by 'nfold'.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58KYneLzgfQ4",
        "outputId": "3ddc9c31-cc40-434b-df23-424f012c7529"
      },
      "source": [
        "#Show results of cross-validation\n",
        "crossval_results\n",
        "\n",
        "#Find the minimum MAE score\n",
        "crossval_results['test-mae-mean'].min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1706622"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyhWRI_qzQB0"
      },
      "source": [
        "Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2_zxkCLFiaet"
      },
      "source": [
        "#Perform crossvalidation for 'max_depth' and 'min_child_weight' \n",
        "#(two parameters that control tree complexity)\n",
        "\n",
        "gridsearch_params = {\n",
        "                     (max_depth, min_child_weight)\n",
        "                     for max_depth in range (6, 10) #5, 15\n",
        "                     for min_child_weight in range (5, 8) #3, 10\n",
        "}\n",
        "\n",
        "\n",
        "# Define initial best params and MAE\n",
        "min_mae = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "min_mae = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "    \n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mae'},\n",
        "        early_stopping_rounds=5 #from 10 to speed up\n",
        "    )\n",
        "    # Update best MAE\n",
        "    mean_mae = cv_results['test-mae-mean'].min()\n",
        "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
        "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
        "    if mean_mae < min_mae:\n",
        "        min_mae = mean_mae\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "'''Best so far: CV with max_depth=6, min_child_weight=6\n",
        "\tMAE 0.16506959999999998 for 97 rounds'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EyY2l_9Yflf"
      },
      "source": [
        "#Update parameters\n",
        "params['max_depth'] = 6\n",
        "params['min_child_weight'] = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6IPCKz6jyWV",
        "outputId": "75d71b7e-20ab-4b88-a903-57812b735bc4"
      },
      "source": [
        "#Final parameters dict\n",
        "print(params)\n",
        "\n",
        "#Tuned model\n",
        "xgbmodel = xgb.train(\n",
        "    params, \n",
        "    dtrain,\n",
        "    num_boost_round = num_boost_round, \n",
        "    evals = [(dtest, \"Test\")],\n",
        "    #obj = custom_mse,\n",
        "    early_stopping_rounds = 10 #number of rounds without improvement before stopping\n",
        ")\n",
        "\n",
        "#Save model training steps to a dataframe\n",
        "#training_log = pd.DataFrame(xgbmodel.evals_result())\n",
        "#training_log['Evaluation Metric'] = xgbmodel.eval_metric()\n",
        "\n",
        "#Evaluating the model\n",
        "#loss = xgbmodel.evaluate(X_test, y_test)\n",
        "\n",
        "#Predicting values for Ca\n",
        "y_pred = xgbmodel.predict(dtest)\n",
        "\n",
        "#The resulting accuracy of the Multi-layer perceptron model:\n",
        "print(\"The prediction accuracy of the Gradient boosted tree model:\")\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 6, 'min_child_weight': 6, 'eta': 0.3, 'subsample': 1, 'colsample_bytree': 1, 'objective': 'reg:squarederror', 'eval_metric': 'mae'}\n",
            "[0]\tTest-mae:0.650807\n",
            "Will train until Test-mae hasn't improved in 10 rounds.\n",
            "[1]\tTest-mae:0.494551\n",
            "[2]\tTest-mae:0.394053\n",
            "[3]\tTest-mae:0.334239\n",
            "[4]\tTest-mae:0.290615\n",
            "[5]\tTest-mae:0.262709\n",
            "[6]\tTest-mae:0.24278\n",
            "[7]\tTest-mae:0.228604\n",
            "[8]\tTest-mae:0.220149\n",
            "[9]\tTest-mae:0.214332\n",
            "[10]\tTest-mae:0.209959\n",
            "[11]\tTest-mae:0.209561\n",
            "[12]\tTest-mae:0.207733\n",
            "[13]\tTest-mae:0.205048\n",
            "[14]\tTest-mae:0.203996\n",
            "[15]\tTest-mae:0.202894\n",
            "[16]\tTest-mae:0.201605\n",
            "[17]\tTest-mae:0.201188\n",
            "[18]\tTest-mae:0.200589\n",
            "[19]\tTest-mae:0.200116\n",
            "[20]\tTest-mae:0.198594\n",
            "[21]\tTest-mae:0.198763\n",
            "[22]\tTest-mae:0.199087\n",
            "[23]\tTest-mae:0.198681\n",
            "[24]\tTest-mae:0.196859\n",
            "[25]\tTest-mae:0.196761\n",
            "[26]\tTest-mae:0.196892\n",
            "[27]\tTest-mae:0.196632\n",
            "[28]\tTest-mae:0.1959\n",
            "[29]\tTest-mae:0.195175\n",
            "[30]\tTest-mae:0.194919\n",
            "[31]\tTest-mae:0.194469\n",
            "[32]\tTest-mae:0.193516\n",
            "[33]\tTest-mae:0.192976\n",
            "[34]\tTest-mae:0.192892\n",
            "[35]\tTest-mae:0.192747\n",
            "[36]\tTest-mae:0.192664\n",
            "[37]\tTest-mae:0.192401\n",
            "[38]\tTest-mae:0.192336\n",
            "[39]\tTest-mae:0.191918\n",
            "[40]\tTest-mae:0.191591\n",
            "[41]\tTest-mae:0.191356\n",
            "[42]\tTest-mae:0.191248\n",
            "[43]\tTest-mae:0.191213\n",
            "[44]\tTest-mae:0.191115\n",
            "[45]\tTest-mae:0.190821\n",
            "[46]\tTest-mae:0.190878\n",
            "[47]\tTest-mae:0.190931\n",
            "[48]\tTest-mae:0.190815\n",
            "[49]\tTest-mae:0.190764\n",
            "[50]\tTest-mae:0.190571\n",
            "[51]\tTest-mae:0.190626\n",
            "[52]\tTest-mae:0.190476\n",
            "[53]\tTest-mae:0.190455\n",
            "[54]\tTest-mae:0.190421\n",
            "[55]\tTest-mae:0.19045\n",
            "[56]\tTest-mae:0.190506\n",
            "[57]\tTest-mae:0.190521\n",
            "[58]\tTest-mae:0.19037\n",
            "[59]\tTest-mae:0.190253\n",
            "[60]\tTest-mae:0.190107\n",
            "[61]\tTest-mae:0.190069\n",
            "[62]\tTest-mae:0.189964\n",
            "[63]\tTest-mae:0.189866\n",
            "[64]\tTest-mae:0.189891\n",
            "[65]\tTest-mae:0.189607\n",
            "[66]\tTest-mae:0.189566\n",
            "[67]\tTest-mae:0.189542\n",
            "[68]\tTest-mae:0.189518\n",
            "[69]\tTest-mae:0.189506\n",
            "[70]\tTest-mae:0.189464\n",
            "[71]\tTest-mae:0.189456\n",
            "[72]\tTest-mae:0.189451\n",
            "[73]\tTest-mae:0.189493\n",
            "[74]\tTest-mae:0.189388\n",
            "[75]\tTest-mae:0.189361\n",
            "[76]\tTest-mae:0.189318\n",
            "[77]\tTest-mae:0.189309\n",
            "[78]\tTest-mae:0.189323\n",
            "[79]\tTest-mae:0.189349\n",
            "[80]\tTest-mae:0.189327\n",
            "[81]\tTest-mae:0.18937\n",
            "[82]\tTest-mae:0.189366\n",
            "[83]\tTest-mae:0.189331\n",
            "[84]\tTest-mae:0.189306\n",
            "[85]\tTest-mae:0.189317\n",
            "[86]\tTest-mae:0.189268\n",
            "[87]\tTest-mae:0.189243\n",
            "[88]\tTest-mae:0.189249\n",
            "[89]\tTest-mae:0.189239\n",
            "[90]\tTest-mae:0.189215\n",
            "[91]\tTest-mae:0.18918\n",
            "[92]\tTest-mae:0.189131\n",
            "[93]\tTest-mae:0.189138\n",
            "[94]\tTest-mae:0.189133\n",
            "[95]\tTest-mae:0.189146\n",
            "[96]\tTest-mae:0.189148\n",
            "[97]\tTest-mae:0.189175\n",
            "[98]\tTest-mae:0.189191\n",
            "[99]\tTest-mae:0.189159\n",
            "[100]\tTest-mae:0.189191\n",
            "[101]\tTest-mae:0.189202\n",
            "[102]\tTest-mae:0.189197\n",
            "Stopping. Best iteration:\n",
            "[92]\tTest-mae:0.189131\n",
            "\n",
            "The prediction accuracy of the Gradient boosted tree model:\n",
            "RÂ²: 0.8219808310163381\n",
            "Variance Regression Score: 0.8220413933789986\n",
            "max_error: 3.445302897002393\n",
            "mean_absolute_error: 0.18919705349687807\n",
            "mean_squared_error: 0.19087004180456754\n",
            "Root mean squared error:  0.43688676084835476\n",
            "median_absolute_error: 0.07387812866908057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYS6qnNgZljT",
        "outputId": "10e07f90-566f-456d-9bb4-9eb78235cc43"
      },
      "source": [
        "#Perform crossvalidation for 'subsample' and 'colsample_bytree' \n",
        "#(two parameters that control the sampling of the dataset \n",
        "#done at each boosting round. Could reduce chances of overfitting)\n",
        "\n",
        "gridsearch_params = [\n",
        "                     (subsample, colsample)\n",
        "                     for subsample in [i/10. for i in range (8, 11)]\n",
        "                     for colsample in [i/10. for i in range (8, 11)]\n",
        "]\n",
        "\n",
        "# Define initial best params and MAE\n",
        "min_mae = float(\"Inf\")\n",
        "best_params = None\n",
        "for subsample, colsample in reversed(gridsearch_params):\n",
        "    print(\"CV with subsample={}, colsample={}\".format(\n",
        "                             subsample,\n",
        "                             colsample))\n",
        "    \n",
        "    # Update our parameters\n",
        "    params['subsample'] = subsample\n",
        "    params['colsample'] = colsample\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mae'},\n",
        "        early_stopping_rounds=5 #from 10 to speed up\n",
        "    )\n",
        "    # Update best MAE\n",
        "    mean_mae = cv_results['test-mae-mean'].min()\n",
        "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
        "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
        "    if mean_mae < min_mae:\n",
        "        min_mae = mean_mae\n",
        "        best_params = (subsample,colsample)\n",
        "\n",
        "print(\"Best parameters: \", best_params[0],  best_params[1], \"MAE: \", min_mae)\n",
        "\n",
        "'''Best Parameters: subsample = 0.8, colsample = 1.0, MAE: 0.1645836'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with subsample=1.0, colsample=1.0\n",
            "\tMAE 0.17069399999999998 for 82 rounds\n",
            "CV with subsample=1.0, colsample=0.9\n",
            "\tMAE 0.17069399999999998 for 82 rounds\n",
            "CV with subsample=1.0, colsample=0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjO4fGpsjhBC"
      },
      "source": [
        "#Update parameters\n",
        "params['subsample'] = 0.8\n",
        "params['colsample'] = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjQjcEAmqYr7"
      },
      "source": [
        "---\n",
        "####**Neural Network (with sklearn)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbNKUpc8qkqh",
        "outputId": "90fa888d-969a-45db-bb9b-4cb849a3fbd0"
      },
      "source": [
        "#Defining the object\n",
        "nn = MLPRegressor()\n",
        "\n",
        "#Fitting/Training the model/object\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "#Predicting values for Ca\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "#print(\"\\n\\n\", nn.loss_)\n",
        "\n",
        "#The resulting accuracy of the Multi-layer perceptron model:\n",
        "print(\"The prediction accuracy of the Multi-layer perceptron model:\")\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The prediction accuracy of the Multi-layer perceptron model:\n",
            "RÂ²: -0.02585909949147358\n",
            "Variance Regression Score: 0.0\n",
            "max_error: 6.981147622236753\n",
            "mean_absolute_error: 0.6872720990310072\n",
            "mean_squared_error: 1.0999139605213197\n",
            "Root mean squared error:  1.0487678296559824\n",
            "median_absolute_error: 0.6069243854226065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEz8Qu0otEj9"
      },
      "source": [
        "###**Attempt Neural Network using Tensorflow**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8kyaCDP66og",
        "outputId": "9ca450cd-e201-43e7-fb93-99be067f03f5"
      },
      "source": [
        "#Defining the model/object parameters\n",
        "ncols = X_train.shape[1]\n",
        "\n",
        "#Initializing the model constructor\n",
        "nn = keras.Sequential()\n",
        "\n",
        "#Add hidden layers\n",
        "nn.add(keras.layers.Dense(20, input_shape = (ncols,), activation = 'sigmoid'))\n",
        "#nn.add(keras.layers.Dense(10, activation = 'relu', kernel_initializer = 'normal'))\n",
        "\n",
        "#Add output layer (one neuron, no activation function)\n",
        "nn.add(keras.layers.Dense(1))\n",
        "\n",
        "nn.summary()\n",
        "\n",
        "\n",
        "\n",
        "#del nn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 20)                71900     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 71,921\n",
            "Trainable params: 71,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hnRyqQdFLa3"
      },
      "source": [
        "#Compiling the model\n",
        "optimizer = keras.optimizers.Adam(0.001)\n",
        "nn.compile(loss = 'mean_squared_error', optimizer = optimizer, metrics = ['mae', 'mse'])\n",
        "\n",
        "#Training/Fitting the model/object\n",
        "training_cycle = 50\n",
        "model_training = nn.fit(\n",
        "    X_train, y_train,\n",
        "    epochs = training_cycle,\n",
        "    validation_data = (X_test, y_test),\n",
        "    #batch_size = 15, \n",
        "    #callbacks = [early_stop],\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "#Save model training steps to a dataframe\n",
        "training_log = pd.DataFrame(model_training.history)\n",
        "training_log['Epoch'] = model_training.epoch\n",
        "\n",
        "#Evaluating the model\n",
        "loss = nn.evaluate(X_test, y_test)\n",
        "\n",
        "#Predicting values for Ca\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "#The resulting accuracy of the Multi-layer perceptron model:\n",
        "print(\"The prediction accuracy of the Multi-layer perceptron model:\")\n",
        "\n",
        "\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))\n",
        "\n",
        "#training_log.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmdgFpij0hvC"
      },
      "source": [
        "Models performance (in error reduction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Eu_YbeACzVUv",
        "outputId": "5784a8a2-9b21-4bd1-f137-afeb31d10a44"
      },
      "source": [
        "#Plotting mean squared error against training cycles/epochs\n",
        "plt.plot(training_log['mse'])\n",
        "plt.plot(training_log['val_mse'])\n",
        "plt.legend((\"Training\", \"Validation\"), loc = 0)\n",
        "\n",
        "'''Training the 1st 50 epochs with batch_size = 15, and a 2nd 50 epochs \n",
        "without batch size resulted in a reduction of mse from over 0.2 at the end \n",
        "of the 1st 50 epochs to below 0.17 at the end of the 2nd 50 epochs'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Training the 1st 50 epochs with batch_size = 15, and a 2nd 50 epochs \\nwithout batch size resulted in a reduction of mse from over 0.2 at the end \\nof the 1st 50 epochs to below 0.17 at the end of the 2nd 50 epochs'"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7LyWwqsLT5e"
      },
      "source": [
        "####Attempting Hyperparameter tuning for NN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sndmv3Uf1gGU"
      },
      "source": [
        "(First Confirm GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yVToZju1koM",
        "outputId": "6ce3714e-b699-4771-e936-7bee4daf4178"
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "from  tensorflow import test\n",
        "device_name = test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('No GPU device found')\n",
        "print('Found Gpu at {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Gpu at /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUjsnu-xL9Tj",
        "outputId": "564e0d09-0837-4916-e136-546d861c4e1e"
      },
      "source": [
        "#importing keras tuner\n",
        "#!pip install keras-tuner --upgrade\n",
        "import keras_tuner as kt\n",
        "\n",
        "#Hyperparameters to test include\n",
        "#Number of hidden layers\n",
        "#Number of units in the (first) hidden layer\n",
        "#Activation\n",
        "#Optimizer\n",
        "#Number of epochs\n",
        "\n",
        "#function to return a model with hyperparameters\n",
        "def create_model(hp):\n",
        "  #define the model/object\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(\n",
        "      hp.Choice('units', [8, 16, 20, 32, 40, 50, 70]), \n",
        "      hp.Choice('activation', ['sigmoid', 'relu'])))#may need to add 'input_shape = (ncols,)'\n",
        "  model.add(keras.layers.Dense(1))\n",
        "  #tune the learning rate\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values = [0.01, 0.001, 0.0001])\n",
        "  #compile the model\n",
        "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate), loss = 'mean_squared_error', metrics = ['mae', 'mse'])\n",
        "  return model\n",
        "\n",
        "#Instantiating a tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    create_model, \n",
        "    objective = 'val_loss', \n",
        "    max_trials = 100\n",
        ")\n",
        "'''Could try any of:\n",
        "The base Tuner class\n",
        "RandomSearch Tuner\n",
        "BayesianOptimization Tuner\n",
        "Hyperband Tuner\n",
        "Sklearn Tuner'''\n",
        "\n",
        "#print a summary of the tuner search space\n",
        "tuner.search_space_summary()\n",
        "\n",
        "#start the search\n",
        "tuner.search(X_train, y_train, epochs = 50, validation_data = (X_test, y_test))\n",
        "#best_model = tuner.get_best_models()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 29 Complete [00h 00m 10s]\n",
            "val_loss: 0.3259114921092987\n",
            "\n",
            "Best val_loss So Far: 0.18618710339069366\n",
            "Total elapsed time: 00h 04m 19s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUId5GGjhfvS"
      },
      "source": [
        "Find out which models performed best in Hyperparameter tuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pls7klg2hadB"
      },
      "source": [
        "#query the results of hyperparameter tuning\n",
        "best_models = tuner.get_best_models()[0]\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "\n",
        "'''print(f\"\"\"\n",
        "The results of the hyperparameter search are as follows:\\n\n",
        "The best model is {best_models.get()}\\n\n",
        "The optimal number of units in the firsr dense (hidden?) layer is {best_hps.get('units')}\\n\n",
        "The best learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\"\"\")'''\n",
        "\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw45DH8Of2DM",
        "outputId": "2c8ce944-4da0-4ad4-bf6e-82abf6d55165"
      },
      "source": [
        "#summary of results\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name='val_loss', direction='min')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 50\n",
            "activation: relu\n",
            "learning_rate: 0.001\n",
            "Score: 0.17906071245670319\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 50\n",
            "activation: sigmoid\n",
            "learning_rate: 0.001\n",
            "Score: 0.18603262305259705\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 50\n",
            "activation: relu\n",
            "learning_rate: 0.0001\n",
            "Score: 0.19444268941879272\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "activation: sigmoid\n",
            "learning_rate: 0.001\n",
            "Score: 0.19786898791790009\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 40\n",
            "activation: relu\n",
            "learning_rate: 0.0001\n",
            "Score: 0.19852083921432495\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 70\n",
            "activation: sigmoid\n",
            "learning_rate: 0.001\n",
            "Score: 0.22694402933120728\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 20\n",
            "activation: sigmoid\n",
            "learning_rate: 0.001\n",
            "Score: 0.2304118126630783\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "activation: relu\n",
            "learning_rate: 0.0001\n",
            "Score: 0.2307625710964203\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 70\n",
            "activation: relu\n",
            "learning_rate: 0.0001\n",
            "Score: 0.2310360223054886\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "activation: sigmoid\n",
            "learning_rate: 0.0001\n",
            "Score: 0.27412843704223633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h0SJurekITD"
      },
      "source": [
        "####**Attempt new model based on results of hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "5ZHhiBqKkX-q",
        "outputId": "80dfb793-b093-4889-ce3d-6858e7389eb9"
      },
      "source": [
        "#1. Defining the model/object parameters\n",
        "ncols = X_train.shape[1]\n",
        "\n",
        "#Initializing the model constructor\n",
        "nn = keras.Sequential()\n",
        "\n",
        "#Add hidden layers\n",
        "nn.add(keras.layers.Dense(50, input_shape = (ncols,), activation = 'relu'))\n",
        "#nn.add(keras.layers.Dense(10, activation = 'relu', kernel_initializer = 'normal'))\n",
        "\n",
        "#Add output layer (one neuron, no activation function)\n",
        "nn.add(keras.layers.Dense(1))\n",
        "\n",
        "nn.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#2.Compiling the model\n",
        "optimizer = keras.optimizers.Adam(0.001)\n",
        "nn.compile(loss = 'mean_squared_error', optimizer = optimizer, metrics = ['mae', 'mse'])\n",
        "\n",
        "#Training/Fitting the model/object\n",
        "training_cycle = 50\n",
        "model_training = nn.fit(\n",
        "    X_train, y_train,\n",
        "    epochs = training_cycle,\n",
        "    validation_data = (X_test, y_test),\n",
        "    #batch_size = 15, \n",
        "    #callbacks = [early_stop],\n",
        "    verbose = False\n",
        ")\n",
        "\n",
        "#Save model training steps to a dataframe\n",
        "training_log = pd.DataFrame(model_training.history)\n",
        "training_log['Epoch'] = model_training.epoch\n",
        "\n",
        "#Evaluating the model\n",
        "loss = nn.evaluate(X_test, y_test)\n",
        "\n",
        "#Predicting values for Ca\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "#The resulting accuracy of the Multi-layer perceptron model:\n",
        "print(\"\\nThe prediction accuracy of the Multi-layer perceptron model:\")\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}\\n'.format(median_absolute_error))\n",
        "\n",
        "#training_log.head()\n",
        "\n",
        "\n",
        "#3. Plotting mean squared error against training cycles/epochs\n",
        "plt.plot(training_log['mse'])\n",
        "plt.plot(training_log['val_mse'])\n",
        "plt.legend((\"Training\", \"Validation\"), loc = 0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                179750    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 179,801\n",
            "Trainable params: 179,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2062 - mae: 0.2889 - mse: 0.2062\n",
            "\n",
            "The prediction accuracy of the Multi-layer perceptron model:\n",
            "RÂ²: 0.8076761140082233\n",
            "Variance Regression Score: 0.8108838715979029\n",
            "max_error: 3.1710566481622076\n",
            "mean_absolute_error: 0.2889291691067613\n",
            "mean_squared_error: 0.20620738973698027\n",
            "Root mean squared error:  0.45410063833579917\n",
            "median_absolute_error: 0.2116471794910263\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f01a5be3f10>"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHb7LNEToHuL"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz2YxgrdoIsP"
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NaVQFTqU8Gz"
      },
      "source": [
        "###**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5XhQNneVBsS"
      },
      "source": [
        "#Defining the model/object\n",
        "lregression = LogisticRegression(multi_class = 'multinomial')\n",
        "\n",
        "#Fitting the model\n",
        "lregression.fit(X_train, y_train)\n",
        "\n",
        "#Calculating probabilities\n",
        "y_pred_probability = lregression.predict_proba(X_test)\n",
        "\n",
        "#Predicting Ca values\n",
        "y_pred = np.argmax(y_pred_probability, axis = 1)\n",
        "\n",
        "#The resulting accuracy of the Linear Regression Model:\n",
        "print(\"The prediction accuracy of the Logistic Regression model:\")\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "print('Root mean squared error: ', mean_squared_error**0.5)\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw6hDcYDtXIM"
      },
      "source": [
        "#Defining the object\n",
        "nn = MLPRegressor()\n",
        "\n",
        "#Grid:\n",
        "parameters = {\n",
        "    \"hidden_layer_sizes\": [(100,), (100, 10), (20, 20), (4, 4)], \n",
        "    \"activation\": ['logistic', 'tanh', 'relu', 'identity'], \n",
        "    \"solver\": ['lbfgs', 'sgd', 'adam'], \n",
        "    \"alpha\": [0.00001, 0.0001, 0.01, 0.1]\n",
        "    }\n",
        "\n",
        "#Test parameters using the 'GridSearchCV' object\n",
        "test = GridSearchCV(nn, parameters)\n",
        "\n",
        "\n",
        "#Fitting the model\n",
        "test.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#Parameters performance scores\n",
        "print(\"Best parameters: \", test.best_params_)\n",
        "print(\"Best score: \", test.best_score_)\n",
        "\n",
        "#Predicting Ca values\n",
        "y_pred = test.predict(X_test)\n",
        "\n",
        "#The resulting accuracy of the test:\n",
        "print(\"The prediction accuracy of the Multi-layer perceptron model after testing/tuning parameters:\")\n",
        "\n",
        "r_square = metrics.r2_score(y_test, y_pred)\n",
        "print('R\\u00b2: {}'.format(r_square))\n",
        "var_explained = metrics.explained_variance_score(y_test, y_pred)\n",
        "print('Variance Regression Score: {}'.format(var_explained))\n",
        "max_error = metrics.max_error(y_test, y_pred)\n",
        "print('max_error: {}'.format(max_error))\n",
        "mean_absolute_error = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print('mean_absolute_error: {}'.format(mean_absolute_error))\n",
        "mean_squared_error = metrics.mean_squared_error(y_test, y_pred)\n",
        "print('mean_squared_error: {}'.format(mean_squared_error))\n",
        "#print('Root mean squared error: ', sqrt(mean_squared_error))\n",
        "median_absolute_error = metrics.median_absolute_error(y_test, y_pred)\n",
        "print('median_absolute_error: {}'.format(median_absolute_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W03q31afkZH3"
      },
      "source": [
        "Attempt to save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykuhbDwc4gtd"
      },
      "source": [
        "import torch #to save and load. \n",
        "#Use another block to load to avoid overriding save\n",
        "\n",
        "model_save_name = 'afsis_SVM.pt'\n",
        "path = F\"/content/gdrive/My Drive/Colab Notebooks/SaveProgress/{model_save_name}\" \n",
        "torch.save(the_model(), path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
